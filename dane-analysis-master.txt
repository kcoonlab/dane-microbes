conda activate qiime2-2021.11

qiime tools import --type 'SampleData[PairedEndSequencesWithQuality]' \
 --input-path manifest.txt \
 --output-path demux.qza \
 --input-format PairedEndFastqManifestPhred33

qiime cutadapt trim-paired \
  --i-demultiplexed-sequences demux.qza \
  --p-front-f GTGCCAGCMGCCGCGGTAA \
  --p-front-r GGACTACHVGGGTWTCTAAT \
  --o-trimmed-sequences primer-trimmed-demux.qza
  
qiime demux summarize \
  --i-data primer-trimmed-demux.qza \
  --o-visualization primer-trimmed-demux.qzv

qiime dada2 denoise-paired \
  --i-demultiplexed-seqs primer-trimmed-demux.qza \
  --p-trunc-len-f 249 \
  --p-trunc-len-r 244 \
  --p-n-threads 0 \
  --o-table table \
  --o-representative-sequences rep-seqs \
  --o-denoising-stats denoise-stats \

qiime feature-table summarize \
  --i-table table.qza \
  --o-visualization table.qzv

qiime feature-table filter-samples \
  --i-table table.qza \
  --m-metadata-file metadata.txt \
  --p-where "[keep]='1'" \
  --o-filtered-table filtered-table.qza

qiime feature-table summarize \
  --i-table filtered-table.qza \
  --o-visualization filtered-table.qzv

library(qiime2R)
library(phyloseq)
library(decontam)

ASVs=read_qza("filtered-table.qza")
metadata=read_q2metadata("metadata2.txt")
taxonomy=read_qza("training-feature-classifiers/taxonomy.qza")
phyloseq=qza_to_phyloseq(
    features="filtered-table.qza",
    tree="rooted-tree.qza",
    "training-feature-classifiers/taxonomy.qza",
    metadata = "metadata2.txt"
    )

contam.freq=isContaminant(phyloseq,method="frequency",conc="quant_reading",threshold=0.5)
contaminants=contam.freq[which(contam.freq$contaminant=="TRUE"),]
write.csv(contaminants,"features-to-discard.csv", row.names = TRUE)

library(PERFect)

df1=otu_table(phyloseq)
df2=t(df1)
res_sim=PERFect_sim(df2)
df3=t(res_sim$filtX)
write.csv(df3[,1],"features-to-keep.csv", row.names = TRUE)

qiime feature-table filter-features \
  --i-table filtered-table.qza \
  --m-metadata-file features-to-keep.txt \
  --o-filtered-table feature-filtered-table.qza

qiime feature-table filter-features \
  --i-table feature-filtered-table.qza \
  --m-metadata-file features-to-discard.txt \
  --p-exclude-ids \
  --o-filtered-table feature-filtered-table2.qza

qiime feature-table summarize \
  --i-table feature-filtered-table2.qza \
  --o-visualization feature-filtered-table2.qzv

qiime feature-table filter-samples \
  --i-table feature-filtered-table2.qza \
  --m-metadata-file metadata2.txt \
  --p-where "[keep]=1" \
  --o-filtered-table sample-feature-filtered-table.qza

qiime feature-table summarize \
  --i-table sample-feature-filtered-table.qza \
  --o-visualization sample-feature-filtered-table.qzv

qiime feature-table group \
  --i-table sample-feature-filtered-table.qza \
  --p-axis sample \
  --m-metadata-file metadata2.txt \
  --m-metadata-column site \
  --p-mode sum \
  --o-grouped-table reindexed-table.qza
  
qiime feature-table summarize \
  --i-table reindexed-table.qza \
  --o-visualization reindexed-table.qzv

qiime phylogeny align-to-tree-mafft-fasttree \
  --i-sequences rep-seqs.qza \
  --p-n-threads auto \
  --o-alignment aligned-rep-seqs.qza \
  --o-masked-alignment masked-aligned-rep-seqs.qza \
  --o-tree unrooted-tree.qza \
  --o-rooted-tree rooted-tree.qza \

#qiime tools export \
#  --input-path rep-seqs.qza \
#  --output-path exported
#
#conda deactivate #De-activate conda environment
#
#cd TaxAss-master/tax-scripts
#
#./RunSteps_quickie.sh otus FreshTrain15Jun2020silva138 silva_nr_v138_taxass 98 80 80 2
#
#./RunStep_16.sh otus FreshTrain15Jun2020silva138 silva_nr_v138_taxass
#
#cd ../
#
#cd ../
#
#conda activate qiime2-2020.11
#
#qiime tools import \
#  --type FeatureData[Taxonomy] \
#  --input-path TaxAss-master/tax-scripts/data/otus.98.80.80.taxonomy \
#  --output-path taxass_taxonomy_98.qza
#  
#qiime metadata tabulate \
#  --m-input-file taxass_taxonomy_98.qza \
#  --o-visualization taxass_taxonomy_98.qzv

mkdir training-feature-classifiers
cd training-feature-classifiers

qiime tools import \
  --type 'FeatureData[Sequence]' \
  --input-path gg_13_8_otus/rep_set/99_otus.fasta \
  --output-path 99_otus.qza

qiime tools import \
  --type 'FeatureData[Taxonomy]' \
  --input-format HeaderlessTSVTaxonomyFormat \
  --input-path gg_13_8_otus/taxonomy/99_otu_taxonomy.txt \
  --output-path ref-taxonomy.qza

qiime feature-classifier extract-reads \
  --i-sequences 99_otus.qza \
  --p-f-primer GTGCCAGCMGCCGCGGTAA \
  --p-r-primer GGACTACHVGGGTWTCTAAT \
  --o-reads ref-seqs.qza

qiime feature-classifier fit-classifier-naive-bayes \
  --i-reference-reads ref-seqs.qza \
  --i-reference-taxonomy ref-taxonomy.qza \
  --o-classifier classifier.qza

cd ../

qiime feature-classifier classify-sklearn \
  --i-classifier training-feature-classifiers/classifier.qza \
  --i-reads rep-seqs.qza \
  --o-classification training-feature-classifiers/taxonomy.qza

qiime metadata tabulate \
  --m-input-file training-feature-classifiers/taxonomy.qza \
  --o-visualization training-feature-classifiers/taxonomy.qzv

qiime taxa filter-table \
  --i-table reindexed-table.qza \
  --i-taxonomy training-feature-classifiers/taxonomy.qza \
  --p-exclude k__Archaea,c__Chloroplast,f__mitochondria \
  --o-filtered-table taxa-filtered-reindexed-table.qza

qiime feature-table summarize \
  --i-table taxa-filtered-reindexed-table.qza \
  --o-visualization taxa-filtered-reindexed-table.qzv

qiime feature-table filter-samples \
  --i-table taxa-filtered-reindexed-table.qza \
  --p-min-frequency 1000 \
  --o-filtered-table analysis-table.qza

qiime feature-table summarize \
  --i-table analysis-table.qza \
  --o-visualization analysis-table.qzv

qiime taxa collapse \
  --i-table analysis-table.qza \
  --i-taxonomy training-feature-classifiers/taxonomy.qza \
  --p-level 2 \
  --o-collapsed-table level2-analysis-table.qza

qiime feature-table relative-frequency \
  --i-table level2-analysis-table.qza \
  --o-relative-frequency-table rel-level2-analysis-table.qza

qiime tools export \
  --input-path rel-level2-analysis-table.qza \
  --output-path exported

biom convert -i exported/feature-table.biom -o rel-level2-analysis-table.tsv --to-tsv

#https://enterotype.embl.de/enterotypes.html

library(cluster)
library(clusterSim)
library(clustertend)
library(factoextra)

#https://rstudio-pubs-static.s3.amazonaws.com/375287_5021917f670c435bb0458af333716136.html

data=read.table("rel-level2-analysis-table.txt", header=T, row.names=1, dec=".", sep="\t")
data=data[-1,]
hopkins=hopkins(data,n=nrow(data)-1)
hopkins
JSD=function(x,y)sqrt(0.5*KLD(x,(x+y)/2)+0.5*KLD(y,(x+y)/2))
KLD=function(x,y)sum(x*log(x/y))
dist.JSD=function(inMatrix,pseudocount=0.000001,...){
  KLD=function(x,y)sum(x*log(x/y))
  JSD=function(x,y)sqrt(0.5*KLD(x,(x+y)/2)+0.5*KLD(y,(x+y)/2))
  matrixColSize=length(colnames(inMatrix))
  matrixRowSize=length(rownames(inMatrix))
  colnames=colnames(inMatrix)
  resultsMatrix=matrix(0,matrixColSize,matrixColSize)
  inMatrix=apply(inMatrix,1:2,function(x)ifelse(x==0,pseudocount,x))
    for(i in 1:matrixColSize){
      for(j in 1:matrixColSize){
        resultsMatrix[i,j]=JSD(as.vector(inMatrix[,i]),
		as.vector(inMatrix[,j]))
		}
	  }
  colnames -> colnames(resultsMatrix) -> rownames(resultsMatrix)
  as.dist(resultsMatrix)->resultsMatrix
  attr(resultsMatrix,"method")="dist"
  return(resultsMatrix) 
  }
data.dist=dist.JSD(data)
pam.clustering=function(x,k) { # x is a distance matrix and k the number of clusters
                         require(cluster)
                         cluster = as.vector(pam(as.dist(x), k, diss=TRUE)$clustering)
                         return(cluster)
                        }
nclusters=NULL
for (k in 1:20) { 
  if (k==1) {
    nclusters[k]=NA 
    } else {
      data.cluster_temp=pam.clustering(data.dist, k)
      nclusters[k]=index.G1(t(data),data.cluster_temp,  d = data.dist,
      centrotypes = "medoids")
      }
	}
plot(nclusters, type="h", xlab="k clusters", ylab="CH index")
noise.removal <- function(dataframe, percent=1, top=NULL){
  dataframe->Matrix
  bigones <- rowSums(Matrix)*100/(sum(rowSums(Matrix))) > percent 
  Matrix_1 <- Matrix[bigones,]
  print(percent)
  return(Matrix_1)
  }
data.denoized=noise.removal(data, percent=1)

library(BiotypeR)
library(ade4)
Dane16S.biotypes=biotyper.data.frame(data.denoized,k=2,manalysis=TRUE)
s.class(Dane16S.biotypes$PCA$li,fac=as.factor(Dane16S.biotypes$biotypes),grid=F)
Dane16S.biotypes$BET$tab

qiime taxa barplot \
  --i-table analysis-table.qza \
  --i-taxonomy training-feature-classifiers/taxonomy.qza \
  --m-metadata-file metadata3.txt \
  --o-visualization taxa-bar-plots.qzv

library(ggplot2)
library(Rmisc)

data=read.csv("level2-taxa.csv",header=TRUE)
cbPalette=hcl.colors(8,palette="RdYlBu")
data$Taxon=factor(data$Taxon,levels=c("Actinobacteria","Bacteroidetes","Cyanobacteria","Firmicutes","Planctomycetes","Proteobacteria","Verrucomicrobia","Other"))
ggplot(data=data,aes(x=as.numeric(as.character(Order)),y=Rel,fill=factor(Taxon)))+geom_bar(stat="identity")+scale_fill_manual(values=cbPalette)

ggplot(data,aes(x=Taxon,y=Rel,fill=as.factor(Hclust)))+geom_boxplot()

#Try above after filtering ASVs that don't achieve 0.5% relative abundance threshold in at least one sample? Also-- remember to rarefy the table first!

qiime feature-table relative-frequency \
  --i-table analysis-table.qza \
  --o-relative-frequency-table rel-analysis-table.qza

qiime tools export \
  --input-path rel-analysis-table.qza \
  --output-path exported

biom convert -i exported/feature-table.biom -o rel-analysis-table.tsv --to-tsv

qiime feature-table filter-features \
  --i-table analysis-table.qza \
  --m-metadata-file features-to-keep2.txt \
  --o-filtered-table feature-filtered-analysis-table.qza

qiime feature-table summarize \
  --i-table feature-filtered-analysis-table.qza \
  --o-visualization feature-filtered-analysis-table.qzv

qiime diversity alpha-rarefaction \
  --i-table feature-filtered-analysis-table.qza \
  --p-max-depth 5000 \
  --p-metrics 'observed_features' \
  --o-visualization alpha-rarefaction-feature-filtered-analysis-table.qzv

qiime feature-table rarefy \
  --i-table feature-filtered-analysis-table.qza \
  --p-sampling-depth 958 \
  --o-rarefied-table rarefied-feature-filtered-analysis-table.qza

qiime tools export \
  --input-path rarefied-feature-filtered-analysis-table.qza \
  --output-path exported

qiime tools export \
  --input-path training-feature-classifiers/taxonomy.qza \
  --output-path exported

biom add-metadata -i exported/feature-table.biom -o table-with-taxonomy2.biom --observation-metadata-fp exported/taxonomy.tsv --sc-separated taxonomy
biom convert -i table-with-taxonomy.biom -o table-with-taxonomy2.tsv --header-key taxonomy --to-tsv

library(PMCMRplus)
library(ggplot2)
data=read.csv("alpha-taxa-div2.csv",header=TRUE)
data.all=data[which(data$Taxon=="all"),]
test.all=kruskalTest(Count~as.factor(Hclust),data=data.all) 
test.all #Significant (p = 0.01957)!
data.actino=data[which(data$Taxon=="actino"),]
test.actino=kruskalTest(Count~as.factor(Hclust),data=data.actino)
test.actino #Significant (p = 0.002542)!
data.cyano=data[which(data$Taxon=="cyano"),]
test.cyano=kruskalTest(Count~as.factor(Hclust),data=data.cyano)
test.cyano #Significant (p < 0.0001)!
data.firm=data[which(data$Taxon=="firm"),]
test.firm=kruskalTest(Count~as.factor(Hclust),data=data.firm)
test.firm #Significant (p < 0.0001)!
data.plancto=data[which(data$Taxon=="plancto"),]
test.plancto=kruskalTest(Count~as.factor(Hclust),data=data.plancto)
test.plancto #Significant (p = 0.03892)!
data.proteo=data[which(data$Taxon=="proteo"),]
test.proteo=kruskalTest(Count~as.factor(Hclust),data=data.proteo)
test.proteo #Significant (p = 0.0002541)!
data.verruco=data[which(data$Taxon=="verruco"),]
test.verruco=kruskalTest(Count~as.factor(Hclust),data=data.verruco)
test.verruco #Not significant (p = 0.5094)!
ggplot(data,aes(x=Taxon,y=Count,fill=as.factor(Hclust)))+geom_boxplot(outlier.shape=NA)

#The enrichment in the relative abundance of Actinobacteria, Cyanobacteria, Planctomycetes in Cluster 2 is accompanied by a corresponding increase in actino-, cyano-, planctomycetal diversity
#The enrichment in the relative abundance of Proteobacteria and Firmicutes in Cluster 1 is accompanied by an increase in proteobacterial and firmicutal diversity
#Total bacterial diversity is overall higher in Cluster 1 than in Cluster 2 sites

qiime diversity-lib faith-pd \
  --i-table rarefied-feature-filtered-analysis-table.qza \
  --i-phylogeny rooted-tree.qza \
  --o-vector faith-pd.qza

qiime tools export \
  --input-path faith-pd.qza \
  --output-path exported

library(qiime2R)
library(phyloseq)

data=qza_to_phyloseq(
    features="rarefied-feature-filtered-analysis-table.qza",
    tree="rooted-tree.qza",
    "training-feature-classifiers/taxonomy.qza",
    metadata = "metadata3.txt"
    )

estimate_richness(data,measures=c("Observed","InvSimpson"))

data=read.csv("alpha-div.csv",header=TRUE)
test.faith=kruskalTest(faith_pd~as.factor(Hclust),data=data) 
test.faith #Not significant (p = 0.1212)!
test.obs=kruskalTest(observed_features~as.factor(Hclust),data=data) 
test.obs #Significant (p = 0.03508)!
test.invsimp=kruskalTest(inv_simpson~as.factor(Hclust),data=data) 
test.invsimp #Not significant (p = 0.08357)!
test.dens=kruskalTest(log10(cfus_per_ml)~as.factor(Hclust),data=data) 
test.dens #Significant (p = 0.008604)!
ggplot(data,aes(x=Hclust,y=log10(cfus_per_ml),fill=as.factor(Hclust)))+geom_boxplot(outlier.shape=NA)

qiime diversity-lib bray-curtis \
  --i-table analysis-table.qza \
  --o-distance-matrix bray-curtis-distance-matrix.qza
  
qiime diversity-lib jaccard \
  --i-table analysis-table.qza \
  --o-distance-matrix jaccard-distance-matrix.qza

qiime diversity-lib weighted-unifrac \
  --i-table analysis-table.qza \
  --i-phylogeny rooted-tree.qza \
  --o-distance-matrix weighted-unifrac-distance-matrix.qza
  
qiime diversity-lib unweighted-unifrac \
  --i-table analysis-table.qza \
  --i-phylogeny rooted-tree.qza \
  --o-distance-matrix unweighted-unifrac-distance-matrix.qza

qiime diversity pcoa \
  --i-distance-matrix bray-curtis-distance-matrix.qza \
  --o-pcoa bray-curtis-pcoa-results.qza

qiime diversity pcoa \
  --i-distance-matrix jaccard-distance-matrix.qza \
  --o-pcoa jaccard-pcoa-results.qza

qiime diversity pcoa \
  --i-distance-matrix weighted-unifrac-distance-matrix.qza \
  --o-pcoa weighted-unifrac-pcoa-results.qza

qiime diversity pcoa \
  --i-distance-matrix unweighted-unifrac-distance-matrix.qza \
  --o-pcoa unweighted-unifrac-pcoa-results.qza

qiime emperor plot \
  --i-pcoa bray-curtis-pcoa-results.qza \
  --m-metadata-file metadata3.txt \
  --o-visualization bray-curtis-emperor-plots.qzv

qiime emperor plot \
  --i-pcoa jaccard-pcoa-results.qza \
  --m-metadata-file metadata3.txt \
  --o-visualization jaccard-emperor-plots.qzv

qiime emperor plot \
  --i-pcoa weighted-unifrac-pcoa-results.qza \
  --m-metadata-file metadata3.txt \
  --o-visualization weighted-unifrac-emperor-plots.qzv

qiime emperor plot \
  --i-pcoa unweighted-unifrac-pcoa-results.qza \
  --m-metadata-file metadata3.txt \
  --o-visualization unweighted-unifrac-emperor-plots.qzv

qiime diversity adonis \
  --i-distance-matrix bray-curtis-distance-matrix.qza \
  --m-metadata-file metadata3.txt \
  --p-formula 'Hclust' \
  --o-visualization bray-curtis-hclust-significance.qzv #Significant (p = 0.001)!

qiime diversity adonis \
  --i-distance-matrix jaccard-distance-matrix.qza \
  --m-metadata-file metadata3.txt \
  --p-formula 'Hclust' \
  --o-visualization jaccard-hclust-significance.qzv #Significant (p = 0.001)!

qiime diversity adonis \
  --i-distance-matrix weighted-unifrac-distance-matrix.qza \
  --m-metadata-file metadata3.txt \
  --p-formula 'Hclust' \
  --o-visualization weighted-unifrac-hclust-significance.qzv #Significant (p = 0.001)!

qiime diversity adonis \
  --i-distance-matrix unweighted-unifrac-distance-matrix.qza \
  --m-metadata-file metadata3.txt \
  --p-formula 'Hclust' \
  --o-visualization unweighted-unifrac-hclust-significance.qzv #Significant (p = 0.001)!

qiime feature-table group \
  --i-table rarefied-feature-filtered-analysis-table.qza \
  --p-axis sample \
  --m-metadata-file metadata3.txt \
  --m-metadata-column site \
  --p-mode sum \
  --o-grouped-table table-for-phylo-analyses.qza

qiime tools export \
  --input-path table-for-phylo-analyses.qza \
  --output-path exported

qiime phylogeny filter-tree \
  --i-tree unrooted-tree.qza \
  --i-table table-for-phylo-analyses.qza \
  --o-filtered-tree filtered-tree.qza \

qiime tools export \
  --input-path filtered-tree.qza \
  --output-path exported-tree

library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
register_google(key = "AIzaSyBxYNOj8Cv7e25nm0k3MvXy2U3IxjBW18c")

states=map_data("state")
counties=map_data("county")
dane_county=wi_county[which(wi_county$subregion=="dane"),]
us_base=ggplot(data=states,mapping=aes(x=long,y=lat,group=group)) + 
  coord_fixed(1.3) + 
  geom_polygon(color="black",fill="gray")
us_base + theme_nothing()
us_base + theme_nothing() + 
  geom_polygon(data=dane_county,aes(x=long,y=lat,fill="orange"),color="white")

dane.data=read.csv("alpha-div.csv",header=TRUE)
dane.data=subset(dane.data,select=-c(Hclust,faith_pd,observed_features,inv_simpson,cfus_per_ml))
ll_means=sapply(dane.data[2:3],mean)
dane_map=get_map(location=ll_means,maptype="terrain-background",source="google",zoom=10)
ggmap(dane_map) + 
  geom_point(data=dane.data,color="red",size=4) +
  geom_text(data=dane.data,aes(label=paste("  ",as.character(AreaNumb),sep="")),angle=60,hjust=0,color="yellow")

data=read.csv("Hist-Master.csv",header=TRUE)
data=data[which(data$AreaNumb==4|data$AreaNumb==5|data$AreaNumb==6|data$AreaNumb==13|data$AreaNumb==26|data$AreaNumb==81|data$AreaNumb==83|data$AreaNumb==86|data$AreaNumb==88|data$AreaNumb==158|data$AreaNumb==164|data$AreaNumb==189|data$AreaNumb==218|data$AreaNumb==243|data$AreaNumb==245|data$AreaNumb==247|data$AreaNumb==249|data$AreaNumb==251|data$AreaNumb==253|data$AreaNumb==254|data$AreaNumb==256|data$AreaNumb==319|data$AreaNumb==335|data$AreaNumb==340|data$AreaNumb==369|data$AreaNumb==371|data$AreaNumb==391|data$AreaNumb==397|data$AreaNumb==417|data$AreaNumb==465|data$AreaNumb==513|data$AreaNumb==515|data$AreaNumb==522|data$AreaNumb==523|data$AreaNumb==526|data$AreaNumb==530|data$AreaNumb==552|data$AreaNumb==556|data$AreaNumb==558|data$AreaNumb==559|data$AreaNumb==565|data$AreaNumb==567|data$AreaNumb==583|data$AreaNumb==584|data$AreaNumb==587|data$AreaNumb==590|data$AreaNumb==597|data$AreaNumb==598|data$AreaNumb==607|data$AreaNumb==615|data$AreaNumb==618|data$AreaNumb==619|data$AreaNumb==620|data$AreaNumb==676|data$AreaNumb==700|data$AreaNumb==715|data$AreaNumb==718|data$AreaNumb==719|data$AreaNumb==724|data$AreaNumb==725|data$AreaNumb==900|data$AreaNumb==969|data$AreaNumb==972|data$AreaNumb==973|data$AreaNumb==975|data$AreaNumb==978|data$AreaNumb==1931|data$AreaNumb==1932|data$AreaNumb==2041|data$AreaNumb==3262|data$AreaNumb==3333|data$AreaNumb==3921|data$AreaNumb==3922|data$AreaNumb==4022|data$AreaNumb==4130|data$AreaNumb==8300|data$AreaNumb==8301|data$AreaNumb==8512|data$AreaNumb==9007|data$AreaNumb==9009|data$AreaNumb==9014|data$AreaNumb==9019|data$AreaNumb==9098|data$AreaNumb==9601|data$AreaNumb==9970|data$AreaNumb==9971),]
write.csv(data,"temp.csv",row.names = FALSE)

data=read.csv("Curr-Master.csv",header=TRUE)
data=data[which(data$AreaNumb==4|data$AreaNumb==5|data$AreaNumb==6|data$AreaNumb==13|data$AreaNumb==26|data$AreaNumb==81|data$AreaNumb==83|data$AreaNumb==86|data$AreaNumb==88|data$AreaNumb==158|data$AreaNumb==164|data$AreaNumb==189|data$AreaNumb==218|data$AreaNumb==243|data$AreaNumb==245|data$AreaNumb==247|data$AreaNumb==249|data$AreaNumb==251|data$AreaNumb==253|data$AreaNumb==254|data$AreaNumb==256|data$AreaNumb==319|data$AreaNumb==335|data$AreaNumb==340|data$AreaNumb==369|data$AreaNumb==371|data$AreaNumb==391|data$AreaNumb==397|data$AreaNumb==417|data$AreaNumb==465|data$AreaNumb==513|data$AreaNumb==515|data$AreaNumb==522|data$AreaNumb==523|data$AreaNumb==526|data$AreaNumb==530|data$AreaNumb==552|data$AreaNumb==556|data$AreaNumb==558|data$AreaNumb==559|data$AreaNumb==565|data$AreaNumb==567|data$AreaNumb==583|data$AreaNumb==584|data$AreaNumb==587|data$AreaNumb==590|data$AreaNumb==597|data$AreaNumb==598|data$AreaNumb==607|data$AreaNumb==615|data$AreaNumb==618|data$AreaNumb==619|data$AreaNumb==620|data$AreaNumb==676|data$AreaNumb==700|data$AreaNumb==715|data$AreaNumb==718|data$AreaNumb==719|data$AreaNumb==724|data$AreaNumb==725|data$AreaNumb==900|data$AreaNumb==969|data$AreaNumb==972|data$AreaNumb==973|data$AreaNumb==975|data$AreaNumb==978|data$AreaNumb==1931|data$AreaNumb==1932|data$AreaNumb==2041|data$AreaNumb==3262|data$AreaNumb==3333|data$AreaNumb==3921|data$AreaNumb==3922|data$AreaNumb==4022|data$AreaNumb==4130|data$AreaNumb==8300|data$AreaNumb==8301|data$AreaNumb==8512|data$AreaNumb==9007|data$AreaNumb==9009|data$AreaNumb==9014|data$AreaNumb==9019|data$AreaNumb==9098|data$AreaNumb==9601|data$AreaNumb==9970|data$AreaNumb==9971),]
write.csv(data,"temp.csv",row.names = FALSE)

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site=sum.by.site[order(sum.by.site$AreaNumb),]
PercSiteDry=100*(sum.by.site$PercSiteDry)

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$SiteDry==0),]
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercTreat=sum.by.site$Treat/sum.by.site$SiteVis
sum.by.site=sum.by.site[order(sum.by.site$AreaNumb),]
TotTreat=sum.by.site$Treat
PercTreat=100*(sum.by.site$PercTreat)

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$SiteDry==0),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$CxFnd=ifelse(data.hist$CxCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist$LarvDens=data.hist$TotCnt/data.hist$NumDips
data.hist$AeDens=data.hist$AeCnt/data.hist$NumDips
data.hist$AnDens=data.hist$AnCnt/data.hist$NumDips
data.hist$CxDens=data.hist$CxCnt/data.hist$NumDips
data.hist$NonVecDens=(data.hist$CxCnt-data.hist$VecCnt)/data.hist$NumDips
data.hist$VecDens=data.hist$VecCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$PercAeFnd=sum.by.site$AeFnd/sum.by.site$SiteVis
sum.by.site$PercAnFnd=sum.by.site$AnFnd/sum.by.site$SiteVis
sum.by.site$PercCxFnd=sum.by.site$CxFnd/sum.by.site$SiteVis
sum.by.site$PercNonVecFnd=sum.by.site$NonVecFnd/sum.by.site$SiteVis
sum.by.site$PercVecFnd=sum.by.site$VecFnd/sum.by.site$SiteVis
sum.by.site=sum.by.site[order(sum.by.site$AreaNumb),]
avg.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
avg.by.site=avg.by.site[order(avg.by.site$AreaNumb),]
PercLarvFnd=100*(sum.by.site$PercLarvFnd)
PercAeFnd=100*(sum.by.site$PercAeFnd)
PercAnFnd=100*(sum.by.site$PercAnFnd)
PercCxFnd=100*(sum.by.site$PercCxFnd)
PercNonVecFnd=100*(sum.by.site$PercNonVecFnd)
PercVecFnd=100*(sum.by.site$PercVecFnd)
AvgLarvDens=avg.by.site$LarvDens
AvgAeDens=avg.by.site$AeDens
AvgAnDens=avg.by.site$AnDens
AvgCxDens=avg.by.site$CxDens
AvgNonVecDens=avg.by.site$NonVecDens
AvgVecDens=avg.by.site$VecDens
AreaNumb=avg.by.site$AreaNumb

df<-data.frame(AreaNumb,PercSiteDry,TotTreat,PercTreat,PercLarvFnd,PercAeFnd,PercAnFnd,PercCxFnd,PercNonVecFnd,PercVecFnd,AvgLarvDens,AvgAeDens,AvgAnDens,AvgCxDens,AvgNonVecDens,AvgVecDens)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,df,by="AreaNumb")
data.total=subset(data.total,select=-c(faith_pd,observed_features,inv_simpson,cfus_per_ml))
write.csv(data.total,"hist-data.csv",row.names = FALSE)

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site=sum.by.site[order(sum.by.site$AreaNumb),]
sum.by.site=sum.by.site[which(sum.by.site$PercSiteDry<1),]
PercSiteDry=100*(sum.by.site$PercSiteDry)

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$SiteDry==0),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercTreat=sum.by.site$Treat/sum.by.site$SiteVis
sum.by.site=sum.by.site[order(sum.by.site$AreaNumb),]
TotTreat=sum.by.site$Treat
PercTreat=100*(sum.by.site$PercTreat)

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$SiteDry==0),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$CxFnd=ifelse(data.curr$CxCnt>0,1,0)
data.curr$NonVecFnd=ifelse((data.curr$CxCnt-data.curr$VecCnt)>0,1,0)
data.curr$VecFnd=ifelse(data.curr$VecCnt>0,1,0)
data.curr$LarvDens=data.curr$TotCnt/data.curr$NumDips
data.curr$AeDens=data.curr$AeCnt/data.curr$NumDips
data.curr$AnDens=data.curr$AnCnt/data.curr$NumDips
data.curr$CxDens=data.curr$CxCnt/data.curr$NumDips
data.curr$NonVecDens=(data.curr$CxCnt-data.curr$VecCnt)/data.curr$NumDips
data.curr$VecDens=data.curr$VecCnt/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$PercAeFnd=sum.by.site$AeFnd/sum.by.site$SiteVis
sum.by.site$PercAnFnd=sum.by.site$AnFnd/sum.by.site$SiteVis
sum.by.site$PercCxFnd=sum.by.site$CxFnd/sum.by.site$SiteVis
sum.by.site$PercNonVecFnd=sum.by.site$NonVecFnd/sum.by.site$SiteVis
sum.by.site$PercVecFnd=sum.by.site$VecFnd/sum.by.site$SiteVis
sum.by.site=sum.by.site[order(sum.by.site$AreaNumb),]
avg.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
avg.by.site=avg.by.site[order(avg.by.site$AreaNumb),]
PercLarvFnd=100*(sum.by.site$PercLarvFnd)
PercAeFnd=100*(sum.by.site$PercAeFnd)
PercAnFnd=100*(sum.by.site$PercAnFnd)
PercCxFnd=100*(sum.by.site$PercCxFnd)
PercNonVecFnd=100*(sum.by.site$PercNonVecFnd)
PercVecFnd=100*(sum.by.site$PercVecFnd)
AvgLarvDens=avg.by.site$LarvDens
AvgAeDens=avg.by.site$AeDens
AvgAnDens=avg.by.site$AnDens
AvgCxDens=avg.by.site$CxDens
AvgNonVecDens=avg.by.site$NonVecDens
AvgVecDens=avg.by.site$VecDens
AreaNumb=avg.by.site$AreaNumb

df<-data.frame(AreaNumb,PercSiteDry,TotTreat,PercTreat,PercLarvFnd,PercAeFnd,PercAnFnd,PercCxFnd,PercNonVecFnd,PercVecFnd,AvgLarvDens,AvgAeDens,AvgAnDens,AvgCxDens,AvgNonVecDens,AvgVecDens)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,df,by="AreaNumb")
write.csv(data.total,"curr-data.csv",row.names = FALSE)

data=read.csv("curr-data.csv",header=TRUE)
data$AreaNumb <- paste('S',data$AreaNumb,sep="")
data2=read.csv("curr-env-data.csv",header=TRUE)
data3=merge(data,data2,by="AreaNumb")
data3$Hclust.x[data3$Hclust.x==1]=0
data3$Hclust.x[data3$Hclust.x==2]=1

model=glm(TotTreat~Hclust.x+faith_pd.x+observed_features.x+inv_simpson.x+cfus_per_ml.x+PercSiteDry+AvgLSTDay+AvgLSTNight+AvgEVI+EvgGpp+AvgNpp,family=poisson(link='log),data=data.total)
summary(model) #Not significant (p = 0.214)!
model=glm(LarvDiv~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.942)!























































data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$LarvFnd==1&data.hist$NumDips>0),]
data.hist$LarvDens=data.hist$TotCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$LarvDens=round(data.total$LarvDens,0)
data.total$LarvDiv=round(data.total$LarvDiv,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(LarvDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.214)!
model=glm(LarvDiv~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.942)!







data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$CxFnd=ifelse(data.hist$CxCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=subset(data.hist,select=-c(DipID,SampDate))
sum.by.site=aggregate(.~AreaNumb+Year,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.2007=data.total[which(data.total$Year==2007&data.total$Hclust==0),]
data.2008=data.total[which(data.total$Year==2008&data.total$Hclust==0),]
data.2009=data.total[which(data.total$Year==2009&data.total$Hclust==0),]
data.2010=data.total[which(data.total$Year==2010&data.total$Hclust==0),]
data.2011=data.total[which(data.total$Year==2011&data.total$Hclust==0),]
data.2012=data.total[which(data.total$Year==2012&data.total$Hclust==0),]
data.2013=data.total[which(data.total$Year==2013&data.total$Hclust==0),]
data.2014=data.total[which(data.total$Year==2014&data.total$Hclust==0),]
data.2015=data.total[which(data.total$Year==2015&data.total$Hclust==0),]
data.2016=data.total[which(data.total$Year==2016&data.total$Hclust==0),]
data.2017=data.total[which(data.total$Year==2017&data.total$Hclust==0),]
data.2018=data.total[which(data.total$Year==2018&data.total$Hclust==0),]
data.2007H1=data.total[which(data.total$Year==2007&data.total$Hclust==1),]
data.2008H1=data.total[which(data.total$Year==2008&data.total$Hclust==1),]
data.2009H1=data.total[which(data.total$Year==2009&data.total$Hclust==1),]
data.2010H1=data.total[which(data.total$Year==2010&data.total$Hclust==1),]
data.2011H1=data.total[which(data.total$Year==2011&data.total$Hclust==1),]
data.2012H1=data.total[which(data.total$Year==2012&data.total$Hclust==1),]
data.2013H1=data.total[which(data.total$Year==2013&data.total$Hclust==1),]
data.2014H1=data.total[which(data.total$Year==2014&data.total$Hclust==1),]
data.2015H1=data.total[which(data.total$Year==2015&data.total$Hclust==1),]
data.2016H1=data.total[which(data.total$Year==2016&data.total$Hclust==1),]
data.2017H1=data.total[which(data.total$Year==2017&data.total$Hclust==1),]
data.2018H1=data.total[which(data.total$Year==2018&data.total$Hclust==1),]

library(FSA)
library(ggplot2)

year=c(2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018)
permanenceH0=c(sum(data.2007$SiteDry)/sum(data.2007$SiteVis),sum(data.2008$SiteDry)/sum(data.2008$SiteVis),sum(data.2009$SiteDry)/sum(data.2009$SiteVis),sum(data.2010$SiteDry)/sum(data.2010$SiteVis),sum(data.2011$SiteDry)/sum(data.2011$SiteVis),sum(data.2012$SiteDry)/sum(data.2012$SiteVis),sum(data.2013$SiteDry)/sum(data.2013$SiteVis),sum(data.2014$SiteDry)/sum(data.2014$SiteVis),sum(data.2015$SiteDry)/sum(data.2015$SiteVis),sum(data.2016$SiteDry)/sum(data.2016$SiteVis),sum(data.2017$SiteDry)/sum(data.2017$SiteVis),sum(data.2018$SiteDry)/sum(data.2018$SiteVis))
permanenceH1=c(sum(data.2007H1$SiteDry)/sum(data.2007H1$SiteVis),sum(data.2008H1$SiteDry)/sum(data.2008H1$SiteVis),sum(data.2009H1$SiteDry)/sum(data.2009H1$SiteVis),sum(data.2010H1$SiteDry)/sum(data.2010H1$SiteVis),sum(data.2011H1$SiteDry)/sum(data.2011H1$SiteVis),sum(data.2012H1$SiteDry)/sum(data.2012H1$SiteVis),sum(data.2013H1$SiteDry)/sum(data.2013H1$SiteVis),sum(data.2014H1$SiteDry)/sum(data.2014H1$SiteVis),sum(data.2015H1$SiteDry)/sum(data.2015H1$SiteVis),sum(data.2016H1$SiteDry)/sum(data.2016H1$SiteVis),sum(data.2017H1$SiteDry)/sum(data.2017H1$SiteVis),sum(data.2018H1$SiteDry)/sum(data.2018H1$SiteVis))
larvfreqH0=c(sum(data.2007$LarvFnd)/sum(data.2007$SiteVis),sum(data.2008$LarvFnd)/sum(data.2008$SiteVis),sum(data.2009$LarvFnd)/sum(data.2009$SiteVis),sum(data.2010$LarvFnd)/sum(data.2010$SiteVis),sum(data.2011$LarvFnd)/sum(data.2011$SiteVis),sum(data.2012$LarvFnd)/sum(data.2012$SiteVis),sum(data.2013$LarvFnd)/sum(data.2013$SiteVis),sum(data.2014$LarvFnd)/sum(data.2014$SiteVis),sum(data.2015$LarvFnd)/sum(data.2015$SiteVis),sum(data.2016$LarvFnd)/sum(data.2016$SiteVis),sum(data.2017$LarvFnd)/sum(data.2017$SiteVis),sum(data.2018$LarvFnd)/sum(data.2018$SiteVis))
larvfreqH1=c(sum(data.2007H1$LarvFnd)/sum(data.2007H1$SiteVis),sum(data.2008H1$LarvFnd)/sum(data.2008H1$SiteVis),sum(data.2009H1$LarvFnd)/sum(data.2009H1$SiteVis),sum(data.2010H1$LarvFnd)/sum(data.2010H1$SiteVis),sum(data.2011H1$LarvFnd)/sum(data.2011H1$SiteVis),sum(data.2012H1$LarvFnd)/sum(data.2012H1$SiteVis),sum(data.2013H1$LarvFnd)/sum(data.2013H1$SiteVis),sum(data.2014H1$LarvFnd)/sum(data.2014H1$SiteVis),sum(data.2015H1$LarvFnd)/sum(data.2015H1$SiteVis),sum(data.2016H1$LarvFnd)/sum(data.2016H1$SiteVis),sum(data.2017H1$LarvFnd)/sum(data.2017H1$SiteVis),sum(data.2018H1$LarvFnd)/sum(data.2018H1$SiteVis))

df<-data.frame(year,permanenceH0,permanenceH1,larvfreqH0,larvfreqH1)

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$LarvFnd==1&data.hist$NumDips>0),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$CxFnd=ifelse(data.hist$CxCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=subset(data.hist,select=-c(DipID,SampDate))
sum.by.site=aggregate(.~AreaNumb+Year,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.2007=data.total[which(data.total$Year==2007&data.total$Hclust==0),]
data.2008=data.total[which(data.total$Year==2008&data.total$Hclust==0),]
data.2009=data.total[which(data.total$Year==2009&data.total$Hclust==0),]
data.2010=data.total[which(data.total$Year==2010&data.total$Hclust==0),]
data.2011=data.total[which(data.total$Year==2011&data.total$Hclust==0),]
data.2012=data.total[which(data.total$Year==2012&data.total$Hclust==0),]
data.2013=data.total[which(data.total$Year==2013&data.total$Hclust==0),]
data.2014=data.total[which(data.total$Year==2014&data.total$Hclust==0),]
data.2015=data.total[which(data.total$Year==2015&data.total$Hclust==0),]
data.2016=data.total[which(data.total$Year==2016&data.total$Hclust==0),]
data.2017=data.total[which(data.total$Year==2017&data.total$Hclust==0),]
data.2018=data.total[which(data.total$Year==2018&data.total$Hclust==0),]
data.2007H1=data.total[which(data.total$Year==2007&data.total$Hclust==1),]
data.2008H1=data.total[which(data.total$Year==2008&data.total$Hclust==1),]
data.2009H1=data.total[which(data.total$Year==2009&data.total$Hclust==1),]
data.2010H1=data.total[which(data.total$Year==2010&data.total$Hclust==1),]
data.2011H1=data.total[which(data.total$Year==2011&data.total$Hclust==1),]
data.2012H1=data.total[which(data.total$Year==2012&data.total$Hclust==1),]
data.2013H1=data.total[which(data.total$Year==2013&data.total$Hclust==1),]
data.2014H1=data.total[which(data.total$Year==2014&data.total$Hclust==1),]
data.2015H1=data.total[which(data.total$Year==2015&data.total$Hclust==1),]
data.2016H1=data.total[which(data.total$Year==2016&data.total$Hclust==1),]
data.2017H1=data.total[which(data.total$Year==2017&data.total$Hclust==1),]
data.2018H1=data.total[which(data.total$Year==2018&data.total$Hclust==1),]

library(FSA)
library(ggplot2)

year=c(2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018)
aefreqH0=c(sum(data.2007$AeFnd)/sum(data.2007$SiteVis),sum(data.2008$AeFnd)/sum(data.2008$SiteVis),sum(data.2009$AeFnd)/sum(data.2009$SiteVis),sum(data.2010$AeFnd)/sum(data.2010$SiteVis),sum(data.2011$AeFnd)/sum(data.2011$SiteVis),sum(data.2012$AeFnd)/sum(data.2012$SiteVis),sum(data.2013$AeFnd)/sum(data.2013$SiteVis),sum(data.2014$AeFnd)/sum(data.2014$SiteVis),sum(data.2015$AeFnd)/sum(data.2015$SiteVis),sum(data.2016$AeFnd)/sum(data.2016$SiteVis),sum(data.2017$AeFnd)/sum(data.2017$SiteVis),sum(data.2018$AeFnd)/sum(data.2018$SiteVis))
aefreqH1=c(sum(data.2007H1$AeFnd)/sum(data.2007H1$SiteVis),sum(data.2008H1$AeFnd)/sum(data.2008H1$SiteVis),sum(data.2009H1$AeFnd)/sum(data.2009H1$SiteVis),sum(data.2010H1$AeFnd)/sum(data.2010H1$SiteVis),sum(data.2011H1$AeFnd)/sum(data.2011H1$SiteVis),sum(data.2012H1$AeFnd)/sum(data.2012H1$SiteVis),sum(data.2013H1$AeFnd)/sum(data.2013H1$SiteVis),sum(data.2014H1$AeFnd)/sum(data.2014H1$SiteVis),sum(data.2015H1$AeFnd)/sum(data.2015H1$SiteVis),sum(data.2016H1$AeFnd)/sum(data.2016H1$SiteVis),sum(data.2017H1$AeFnd)/sum(data.2017H1$SiteVis),sum(data.2018H1$AeFnd)/sum(data.2018H1$SiteVis))
anfreqH0=c(sum(data.2007$AnFnd)/sum(data.2007$SiteVis),sum(data.2008$AnFnd)/sum(data.2008$SiteVis),sum(data.2009$AnFnd)/sum(data.2009$SiteVis),sum(data.2010$AnFnd)/sum(data.2010$SiteVis),sum(data.2011$AnFnd)/sum(data.2011$SiteVis),sum(data.2012$AnFnd)/sum(data.2012$SiteVis),sum(data.2013$AnFnd)/sum(data.2013$SiteVis),sum(data.2014$AnFnd)/sum(data.2014$SiteVis),sum(data.2015$AnFnd)/sum(data.2015$SiteVis),sum(data.2016$AnFnd)/sum(data.2016$SiteVis),sum(data.2017$AnFnd)/sum(data.2017$SiteVis),sum(data.2018$AnFnd)/sum(data.2018$SiteVis))
anfreqH1=c(sum(data.2007H1$AnFnd)/sum(data.2007H1$SiteVis),sum(data.2008H1$AnFnd)/sum(data.2008H1$SiteVis),sum(data.2009H1$AnFnd)/sum(data.2009H1$SiteVis),sum(data.2010H1$AnFnd)/sum(data.2010H1$SiteVis),sum(data.2011H1$AnFnd)/sum(data.2011H1$SiteVis),sum(data.2012H1$AnFnd)/sum(data.2012H1$SiteVis),sum(data.2013H1$AnFnd)/sum(data.2013H1$SiteVis),sum(data.2014H1$AnFnd)/sum(data.2014H1$SiteVis),sum(data.2015H1$AnFnd)/sum(data.2015H1$SiteVis),sum(data.2016H1$AnFnd)/sum(data.2016H1$SiteVis),sum(data.2017H1$AnFnd)/sum(data.2017H1$SiteVis),sum(data.2018H1$AnFnd)/sum(data.2018H1$SiteVis))
cxfreqH0=c(sum(data.2007$CxFnd)/sum(data.2007$SiteVis),sum(data.2008$CxFnd)/sum(data.2008$SiteVis),sum(data.2009$CxFnd)/sum(data.2009$SiteVis),sum(data.2010$CxFnd)/sum(data.2010$SiteVis),sum(data.2011$CxFnd)/sum(data.2011$SiteVis),sum(data.2012$CxFnd)/sum(data.2012$SiteVis),sum(data.2013$CxFnd)/sum(data.2013$SiteVis),sum(data.2014$CxFnd)/sum(data.2014$SiteVis),sum(data.2015$CxFnd)/sum(data.2015$SiteVis),sum(data.2016$CxFnd)/sum(data.2016$SiteVis),sum(data.2017$CxFnd)/sum(data.2017$SiteVis),sum(data.2018$CxFnd)/sum(data.2018$SiteVis))
cxfreqH1=c(sum(data.2007H1$CxFnd)/sum(data.2007H1$SiteVis),sum(data.2008H1$CxFnd)/sum(data.2008H1$SiteVis),sum(data.2009H1$CxFnd)/sum(data.2009H1$SiteVis),sum(data.2010H1$CxFnd)/sum(data.2010H1$SiteVis),sum(data.2011H1$CxFnd)/sum(data.2011H1$SiteVis),sum(data.2012H1$CxFnd)/sum(data.2012H1$SiteVis),sum(data.2013H1$CxFnd)/sum(data.2013H1$SiteVis),sum(data.2014H1$CxFnd)/sum(data.2014H1$SiteVis),sum(data.2015H1$CxFnd)/sum(data.2015H1$SiteVis),sum(data.2016H1$CxFnd)/sum(data.2016H1$SiteVis),sum(data.2017H1$CxFnd)/sum(data.2017H1$SiteVis),sum(data.2018H1$CxFnd)/sum(data.2018H1$SiteVis))
nonvecfreqH0=c(sum(data.2007$NonVecFnd)/sum(data.2007$SiteVis),sum(data.2008$NonVecFnd)/sum(data.2008$SiteVis),sum(data.2009$NonVecFnd)/sum(data.2009$SiteVis),sum(data.2010$NonVecFnd)/sum(data.2010$SiteVis),sum(data.2011$NonVecFnd)/sum(data.2011$SiteVis),sum(data.2012$NonVecFnd)/sum(data.2012$SiteVis),sum(data.2013$NonVecFnd)/sum(data.2013$SiteVis),sum(data.2014$NonVecFnd)/sum(data.2014$SiteVis),sum(data.2015$NonVecFnd)/sum(data.2015$SiteVis),sum(data.2016$NonVecFnd)/sum(data.2016$SiteVis),sum(data.2017$NonVecFnd)/sum(data.2017$SiteVis),sum(data.2018$NonVecFnd)/sum(data.2018$SiteVis))
nonvecfreqH1=c(sum(data.2007H1$NonVecFnd)/sum(data.2007H1$SiteVis),sum(data.2008H1$NonVecFnd)/sum(data.2008H1$SiteVis),sum(data.2009H1$NonVecFnd)/sum(data.2009H1$SiteVis),sum(data.2010H1$NonVecFnd)/sum(data.2010H1$SiteVis),sum(data.2011H1$NonVecFnd)/sum(data.2011H1$SiteVis),sum(data.2012H1$NonVecFnd)/sum(data.2012H1$SiteVis),sum(data.2013H1$NonVecFnd)/sum(data.2013H1$SiteVis),sum(data.2014H1$NonVecFnd)/sum(data.2014H1$SiteVis),sum(data.2015H1$NonVecFnd)/sum(data.2015H1$SiteVis),sum(data.2016H1$NonVecFnd)/sum(data.2016H1$SiteVis),sum(data.2017H1$NonVecFnd)/sum(data.2017H1$SiteVis),sum(data.2018H1$NonVecFnd)/sum(data.2018H1$SiteVis))
vecfreqH0=c(sum(data.2007$VecFnd)/sum(data.2007$SiteVis),sum(data.2008$VecFnd)/sum(data.2008$SiteVis),sum(data.2009$VecFnd)/sum(data.2009$SiteVis),sum(data.2010$VecFnd)/sum(data.2010$SiteVis),sum(data.2011$VecFnd)/sum(data.2011$SiteVis),sum(data.2012$VecFnd)/sum(data.2012$SiteVis),sum(data.2013$VecFnd)/sum(data.2013$SiteVis),sum(data.2014$VecFnd)/sum(data.2014$SiteVis),sum(data.2015$VecFnd)/sum(data.2015$SiteVis),sum(data.2016$VecFnd)/sum(data.2016$SiteVis),sum(data.2017$VecFnd)/sum(data.2017$SiteVis),sum(data.2018$VecFnd)/sum(data.2018$SiteVis))
vecfreqH1=c(sum(data.2007H1$VecFnd)/sum(data.2007H1$SiteVis),sum(data.2008H1$VecFnd)/sum(data.2008H1$SiteVis),sum(data.2009H1$VecFnd)/sum(data.2009H1$SiteVis),sum(data.2010H1$VecFnd)/sum(data.2010H1$SiteVis),sum(data.2011H1$VecFnd)/sum(data.2011H1$SiteVis),sum(data.2012H1$VecFnd)/sum(data.2012H1$SiteVis),sum(data.2013H1$VecFnd)/sum(data.2013H1$SiteVis),sum(data.2014H1$VecFnd)/sum(data.2014H1$SiteVis),sum(data.2015H1$VecFnd)/sum(data.2015H1$SiteVis),sum(data.2016H1$VecFnd)/sum(data.2016H1$SiteVis),sum(data.2017H1$VecFnd)/sum(data.2017H1$SiteVis),sum(data.2018H1$VecFnd)/sum(data.2018H1$SiteVis))

df<-data.frame(year,aefreqH0,aefreqH1,anfreqH0,anfreqH1,cxfreqH0,cxfreqH1,nonvecfreqH0,nonvecfreqH1,vecfreqH0,vecfreqH1)










Sum=Summarize(PercLarvFnd~Hclust,data=data.total)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Year,y=mean,color=Hclust,data=Sum)+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))



data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0),]
data.hist=subset(data.hist,select=-c(DipID,SampDate))
sum.by.site=aggregate(.~AreaNumb+Year,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total2=data.total[which(data.total$Year==2018),]
model=glm(Hclust~PercLarvFnd+PercSiteDry,family=binomial(link='logit'),data=data.total2) 
summary(model) #Not significant (p = 0.41)!



model<-glm(PercLarvFnd~Hclust+PercSiteDry,family=binomial(link='logit'),weights=SiteVis,data=data.total)
summary(model)


library(geepack)
library(car)
library(MESS)

data.total=data.total[which(data.total$Year==2018),]
fit.1<-geeglm(PercLarvFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.total,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercLarvFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.total,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercLarvFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.total,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.total,weights=SiteVis)
summary(fit.2) #Hclust and Hclust*Year become significant when only 2016, 2017, and/or 2018 are included in the model

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$CxFnd=ifelse(data.hist$CxCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=subset(data.hist,select=-c(DipID,SampDate))
sum.by.site=aggregate(.~AreaNumb+Year,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0 
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis 
sum.by.site$PercAeFnd=sum.by.site$AeFnd/sum.by.site$SiteVis
sum.by.site$PercAnFnd=sum.by.site$AnFnd/sum.by.site$SiteVis
sum.by.site$PercCxFnd=sum.by.site$CxFnd/sum.by.site$SiteVis
sum.by.site$PercNonVecFnd=sum.by.site$NonVecFnd/sum.by.site$SiteVis
sum.by.site$PercVecFnd=sum.by.site$VecFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$Wave=data.total$Year
data.total$Wave[data.total$Wave==2016]=1
data.total$Wave[data.total$Wave==2017]=2
data.total$Wave[data.total$Wave==2018]=3
data.total=data.total[order(data.total$AreaNumb,data.total$Wave),]

fit.1<-geeglm(PercAeFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.total,weights=NumDips)
summary(fit.1)
fit.2<-geeglm(PercAeFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.total,weights=NumDips)
summary(fit.2)
fit.3<-geeglm(PercAeFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.total,weights=NumDips)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~Hclust*Year+PercSiteDry,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.total,weights=SiteVis)
summary(fit.2) #Hclust and Hclust*Year become significant when only 2016, 2017, and/or 2018 are included in the model




data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0),]
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercSiteDry~Hclust,family=binomial(link='logit'),weights=SiteVis,data=data.total)
summary(model) #Significant (p = 0.000347)!
control=exp(-1.22988)/(1+exp(-1.22988))
100*(exp(-0.39483)/(1-control+(control*exp(-0.39483)))-1) #Habitats from Cluster #2 are 27.25263% less likely to be dry than habitats from Cluster #1 

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$SiteDry==0),]
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$Treat=ifelse(sum.by.site$Treat>0,1,0)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercLarvFnd~Hclust,family=binomial(link='logit'),weights=SiteVis,data=data.total)
summary(model) #Significant (p < 0.0001)!
control=exp(-0.51365)/(1+exp(-0.51365))
100*(exp(-0.54496)/(1-control+(control*exp(-0.54496)))-1) #Even when water is present, you are 31.19187% less likely to find larvae in habitats with microbiota of biotype #2 
model=glm(Treat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Significant (p = 0.0164)!
100*(1-exp(-1.3218)) #Cluster 2 habitats are associated with a 73.33451% reduction in the relative risk of being treated
model=glm(Retreat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Not significant (p = 1)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$LarvFnd==1&data.hist$NumDips>0),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$CxFnd=ifelse(data.hist$CxCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercAeFnd=sum.by.site$AeFnd/sum.by.site$SiteVis
sum.by.site$PercAnFnd=sum.by.site$AnFnd/sum.by.site$SiteVis
sum.by.site$PercCxFnd=sum.by.site$CxFnd/sum.by.site$SiteVis
sum.by.site$PercNonVecFnd=sum.by.site$NonVecFnd/sum.by.site$SiteVis
sum.by.site$PercVecFnd=sum.by.site$VecFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercAeFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p < 0.0001)!
control=exp(-1.48975)/(1+exp(-1.48975))
100*(exp(-0.95363)/(1-control+(control*exp(-0.95363)))-1) #You are 56.55342% less likely to find Aedes larvae in habitats with microbiota of biotype #2
model=glm(PercAnFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p = 0.001)!
control=exp(-3.1893)/(1+exp(-3.1893))
100*(exp(1.4919)/(1-control+(control*exp(1.4919)))-1) #You are 291.2148% more likely to find Anopheles larvae in habitats with microbiota of biotype #2
model=glm(PercCxFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p = 0.0353)!
control=exp(1.41307)/(1+exp(1.40307))
100*(exp(-0.19222)/(1-control+(control*exp(-0.19222)))-1) #You are 3.856483% less likely to find Culex larvae in habitats with microbiota of biotype #2
model=glm(PercNonVecFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p < 0.0001)!
control=exp(-2.34664)/(1+exp(-2.34664))
100*(exp(0.68260)/(1-control+(control*exp(0.68260)))-1) #You are 82.31371% more likely to find non-vector Culex larvae in habitats with microbiota of biotype #2
model=glm(PercVecFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p < 0.0001)!
control=exp(1.12465)/(1+exp(1.12465))
100*(exp(-0.65051)/(1-control+(control*exp(-0.65051)))-1) #You are 18.3463% less likely to find vector Culex larvae in habitats with microbiota of biotype #2

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$LarvFnd==1&data.hist$NumDips>0),]
data.hist$LarvDens=data.hist$TotCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$LarvDens=round(data.total$LarvDens,0)
data.total$LarvDiv=round(data.total$LarvDiv,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(LarvDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.214)!
model=glm(LarvDiv~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.942)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$AeFnd==1&data.hist$NumDips>0),]
data.hist$AeDens=data.hist$AeCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$AeDens=round(data.total$AeDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(AeDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.0578)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$AnFnd==1&data.hist$NumDips>0),]
data.hist$AnDens=data.hist$AnCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$AnDens=round(data.total$AnDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(AnDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.216)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$CxFnd=ifelse(data.hist$CxCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$AnFnd==1&data.hist$NumDips>0),]
data.hist$CxDens=data.hist$CxCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$CxDens=round(data.total$CxDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(CxDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.348)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$NonVecFnd==1&data.hist$NumDips>0),]
data.hist$NonVecDens=(data.hist$CxCnt-data.hist$VecCnt)/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$NonVecDens=round(data.total$NonVecDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(NonVecDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.818)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist$AeFnd=ifelse(data.hist$AeCnt>0,1,0)
data.hist$AnFnd=ifelse(data.hist$AnCnt>0,1,0)
data.hist$NonVecFnd=ifelse((data.hist$CxCnt-data.hist$VecCnt)>0,1,0)
data.hist$VecFnd=ifelse(data.hist$VecCnt>0,1,0)
data.hist=data.hist[which(data.hist$Posttreat==0&data.hist$VecFnd==1&data.hist$NumDips>0),]
data.hist$VecDens=data.hist$VecCnt/data.hist$NumDips
data.hist[is.na(data.hist)]=0
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$VecDens=round(data.total$VecDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(VecDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Significant (p < 0.0001)!
100*exp(0.62582) #Being in Cluster 2 is associated with an increase of 186.9779% in Culex vector larval density

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$LarvFnd==1),]
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site$Treat=ifelse(sum.by.site$Treat>0,1,0)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(Treat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Significant (p = 0.031)!
100*(1-exp(-1.3134)) #Cluster 2 habitats are associated with a 73.10958% reduction in the relative risk of being treated

data=read.csv("Combined-Master.csv",header=TRUE)
data.hist=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data.hist=data.hist[which(data.hist$Posttreat==1&data.hist$LarvFnd==1),]
data.hist=subset(data.hist,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.hist,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(Retreat~Hclust,family=binomial(link='logit'),data=data.total)
summary(model) #Not significant (p = 0.653443)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercSiteDry~Hclust,family=binomial(link='logit'),weights=SiteVis,data=data.total)
summary(model) #Not significant (p = 0.29)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$SiteDry==0),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$Treat=ifelse(sum.by.site$Treat>0,1,0)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercLarvFnd~Hclust,family=binomial(link='logit'),weights=SiteVis,data=data.total)
summary(model) #Significant (p = 0.00311)!
control=exp(-0.22086)/(1+exp(-0.22086))
100*(exp(-0.67589)/(1-control+(control*exp(-0.67589)))-1) #Even when water is present, you are 31.19187% less likely to find larvae in habitats with microbiota of biotype #2 
model=glm(Treat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Not significant (p = 0.401)!
model=glm(Retreat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Not significant (p = 1)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$LarvFnd==1&data.curr$NumDips>0),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$NonVecFnd=ifelse((data.curr$CxCnt-data.curr$VecCnt)>0,1,0)
data.curr$VecFnd=ifelse(data.curr$VecCnt>0,1,0)
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercAeFnd=sum.by.site$AeFnd/sum.by.site$SiteVis
sum.by.site$PercAnFnd=sum.by.site$AnFnd/sum.by.site$SiteVis
sum.by.site$PercNonVecFnd=sum.by.site$NonVecFnd/sum.by.site$SiteVis
sum.by.site$PercVecFnd=sum.by.site$VecFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercAeFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p < 0.00085)!
control=exp(-1.18210)/(1+exp(-1.18210))
100*(exp(-0.63319)/(1-control+(control*exp(-0.63319)))-1) #You are 40.34298% less likely to find Aedes larvae in habitats with microbiota of biotype #2
model=glm(PercAnFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p = 0.0238)!
control=exp(-2.9948)/(1+exp(-2.9948))
100*(exp(0.5740)/(1-control+(control*exp(0.5740)))-1) #You are 71.20852% more likely to find Anopheles larvae in habitats with microbiota of biotype #2
model=glm(PercNonVecFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p < 0.0001)!
control=exp(-1.99468)/(1+exp(-1.99468))
100*(exp(1.13210)/(1-control+(control*exp(1.13210)))-1) #You are 147.8241% more likely to find non-vector Culex larvae in habitats with microbiota of biotype #2
model=glm(PercVecFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p = 0.000346)!
control=exp(0.61189)/(1+exp(0.61189))
100*(exp(-0.48371)/(1-control+(control*exp(-0.48371)))-1) #You are 17.94813% less likely to find vector Culex larvae in habitats with microbiota of biotype #2

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$LarvFnd==1&data.curr$NumDips>0),]
data.curr$LarvDens=data.curr$TotCnt/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$LarvDens=round(data.total$LarvDens,0)
data.total$LarvDiv=round(data.total$LarvDiv,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(LarvDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Significant (p < 0.0001)!
model=glm(LarvDiv~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.74)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$NonVecFnd=ifelse((data.curr$CxCnt-data.curr$VecCnt)>0,1,0)
data.curr$VecFnd=ifelse(data.curr$VecCnt>0,1,0)
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$AeFnd==1&data.curr$NumDips>0),]
data.curr$AeDens=data.curr$AeCnt/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$AeDens=round(data.total$AeDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(AeDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.988)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$NonVecFnd=ifelse((data.curr$CxCnt-data.curr$VecCnt)>0,1,0)
data.curr$VecFnd=ifelse(data.curr$VecCnt>0,1,0)
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$AnFnd==1&data.curr$NumDips>0),]
data.curr$AnDens=data.curr$AnCnt/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$AnDens=round(data.total$AnDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(AnDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 1)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$NonVecFnd=ifelse((data.curr$CxCnt-data.curr$VecCnt)>0,1,0)
data.curr$VecFnd=ifelse(data.curr$VecCnt>0,1,0)
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$NonVecFnd==1&data.curr$NumDips>0),]
data.curr$NonVecDens=(data.curr$CxCnt-data.curr$VecCnt)/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$NonVecDens=round(data.total$NonVecDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(NonVecDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Significant (p = 0.00773)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$NonVecFnd=ifelse((data.curr$CxCnt-data.curr$VecCnt)>0,1,0)
data.curr$VecFnd=ifelse(data.curr$VecCnt>0,1,0)
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$VecFnd==1&data.curr$NumDips>0),]
data.curr$VecDens=data.curr$VecCnt/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$VecDens=round(data.total$VecDens,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(VecDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Significant (p < 0.0001)!
100*exp(1.43202) #Being in Cluster 2 is associated with an increase of 418.7179% in Culex vector larval density

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$LarvFnd==1),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site$Treat=ifelse(sum.by.site$Treat>0,1,0)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(Treat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Not significant (p = 0.697)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==1&data.curr$LarvFnd==1),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(Retreat~Hclust,family=binomial(link='logit'),data=data.total)
summary(model) #Not significant (p = 0.995956)!






data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$SiteDry==0),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercLarvFnd~Hclust,family=binomial(link='logit'),weights=SiteVis,data=data.total)
summary(model) #Significant (p = 0.00311)!
control=exp(-0.22086)/(1+exp(-0.22086))
100*(exp(-0.67589)/(1-control+(control*exp(-0.67589)))-1) #Even when water is present, you are 34.89587% less likely to find larvae in habitats with microbiota of biotype #2 

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$LarvFnd==1&data.curr$NumDips>0),]
data.curr$AeFnd=ifelse(data.curr$AeCnt>0,1,0)
data.curr$AnFnd=ifelse(data.curr$AnCnt>0,1,0)
data.curr$CxFnd=ifelse(data.curr$CxCnt>0,1,0)
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercAeFnd=sum.by.site$AeFnd/sum.by.site$SiteVis
sum.by.site$PercAnFnd=sum.by.site$AnFnd/sum.by.site$SiteVis
sum.by.site$PercCxFnd=sum.by.site$CxFnd/sum.by.site$SiteVis
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(PercAeFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p = 0.00085)!
control=exp(-1.18210)/(1+exp(-1.18210))
100*(exp(-0.63319)/(1-control+(control*exp(-0.63319)))-1) #You are 40.34298% less likely to find Aedes larvae in habitats with microbiota of biotype #2
model=glm(PercAnFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Significant (p = 0.0238)!
control=exp(-2.9948)/(1+exp(-2.9948))
100*(exp(0.5740)/(1-control+(control*exp(0.5740)))-1) #You are 71.20852% more likely to find Anopheles larvae in habitats with microbiota of biotype #2
model=glm(PercCxFnd~Hclust,family=binomial(link='logit'),weights=NumDips,data=data.total)
summary(model) #Not significant (p = 0.833)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==0&data.curr$LarvFnd==1&data.curr$NumDips>0),]
data.curr$LarvDens=data.curr$TotCnt/data.curr$NumDips
data.curr$AeDens=data.curr$AeCnt/data.curr$NumDips
data.curr$AnDens=data.curr$AnCnt/data.curr$NumDips
data.curr$CxDens=data.curr$CxCnt/data.curr$NumDips
data.curr[is.na(data.curr)]=0
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) mean(x,na.rm=TRUE),na.action=na.pass)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
data.total$LarvDens=round(data.total$LarvDens,0)
data.total$AeDens=round(data.total$AeDens,0)
data.total$AnDens=round(data.total$AnDens,0)
data.total$CxDens=round(data.total$CxDens,0)
data.total$LarvDiv=round(data.total$LarvDiv,0)
data.total$NumDips=round(data.total$NumDips,0)
model=glm(LarvDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Significant (p < 0.0001)!
100*(exp(1.28054)-1) #Being in Cluster 2 is associated with an increase of 259.8582% in overall larval density
model=glm(AeDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.986)!
model=glm(AnDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 1)!
model=glm(CxDens~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Significant (p < 0.0001)!
100*(exp(1.60244)-1) #Being in Cluster 2 is associated with an increase of 396.5133% in Culex larval density
model=glm(LarvDiv~Hclust,family=poisson(link='log'),offset=log(NumDips),data=data.total)
summary(model) #Not significant (p = 0.74)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$LarvFnd==1),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site$Treat=ifelse(sum.by.site$Treat>0,1,0)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(Treat~Hclust,family=binomial(link='logit'),data=data.total) 
summary(model) #Not significant (p = 0.697)!

data=read.csv("Combined-Master.csv",header=TRUE)
data.curr=data[which(data$Year==2019|data$Year==2020|data$Year==2021),]
data.curr=data.curr[which(data.curr$Posttreat==1&data.curr$LarvFnd==1),]
data.curr=subset(data.curr,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data.curr,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site$Retreat=ifelse(sum.by.site$Retreat>0,1,0)
clust.data=read.csv("alpha-div.csv",header=TRUE)
data.total=merge(clust.data,sum.by.site,by="AreaNumb")
data.total$Hclust[data.total$Hclust==1]=0
data.total$Hclust[data.total$Hclust==2]=1
model=glm(Retreat~Hclust,family=binomial(link='logit'),data=data.total)
summary(model) #Not significant (p = 0.995956)!
















data$Wave=data$Year
data$Wave[data$Wave==2007]=1
data$Wave[data$Wave==2008]=2
data$Wave[data$Wave==2009]=3
data$Wave[data$Wave==2010]=4
data$Wave[data$Wave==2011]=5
data$Wave[data$Wave==2012]=6
data$Wave[data$Wave==2013]=7
data$Wave[data$Wave==2014]=8
data$Wave[data$Wave==2015]=9
data$Wave[data$Wave==2016]=10
data$Wave[data$Wave==2017]=11
data$Wave[data$Wave==2018]=12
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

data$Hclust[data$Hclust==1]=0
data$Hclust[data$Hclust==2]=1

fit.1<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

















data=read.csv("Combined-Master.csv",header=TRUE)
data=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data=data[which(data$Posttreat==0&data$LarvFnd==1),]
data=subset(data,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
data=read.csv("Combined-Master.csv",header=TRUE)
data=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]

data=subset(data,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0 
data.total=merge(data.new,sum.by.site,by="AreaNumb")






data$LarvDens=data$TotCnt/data$NumDips
data$VecDens=data$VecCnt/data$NumDips
data$NonVecDens=(data$TotCnt-data$VecCnt)/data$NumDips
data$AeDens=data$AeCnt/data$NumDips
data$AnDens=data$AnCnt/data$NumDips
data$CxDens=data$CxCnt/data$NumDips
data$DivDens=data$LarvDiv/data$NumDips
data$LarvDens[is.nan(data$LarvDens)]=0
data$VecDens[is.nan(data$VecDens)]=0
data$NonVecDens[is.nan(data$NonVecDens)]=0
data$AeDens[is.nan(data$AeDens)]=0
data$AnDens[is.nan(data$AnDens)]=0
data$CxDens[is.nan(data$CxDens)]=0
data$DivDens[is.nan(data$DivDens)]=0



sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis









sum.by.site$LarvDens=sum.by.site$TotCnt/sum.by.site$NumDips
sum.by.site$LarvDens[is.nan(sum.by.site$LarvDens)]=0



data.total$PercRetreat=data.total$Retreat/data.total$SiteVis
data.total$PercRetreat2=data.total$Retreat/data.total$Posttreat
data.total$PercRetreat2[is.nan(data.total$PercRetreat2)]=0
data.total$Treat[data.total$Treat>0]=1
data.total$Retreat[data.total$Retreat>0]=1
data.alpha=read.csv("alpha-div.csv",header=TRUE)
data.analysis=merge(data.total,data.alpha,by="AreaNumb")
data.analysis$Hclust[data.analysis$Hclust==1]=0
data.analysis$Hclust[data.analysis$Hclust==2]=1
data.analysis$Pretreat=data.analysis$SiteVis-data.analysis$Posttreat
data.analysis$PercentSiteDry=(data.analysis$SiteDry/data.analysis$SiteVis)*100
data.analysis$PercentLarvFnd=data.analysis$PercLarvFnd*100
data.analysis$PercTreat=data.analysis$Treat/data.analysis$SiteVis
data.analysis$PercentTreat=(data.analysis$Treat/data.analysis$SiteVis)*100
data.analysis$PercentRetreat=data.analysis$PercRetreat*100
data.analysis$PercentRetreat2=data.analysis$PercRetreat2*100

wilcox.test(log10(observed_features)~as.factor(Hclust),data=data.analysis) #Significant (p = 0.01)!
wilcox.test(log10(faith_pd)~as.factor(Hclust),data=data.analysis) #Significant (p = 0.03)!
wilcox.test(pielou_evenness~as.factor(Hclust),data=data.analysis) #Significant (p = 0.04)!
wilcox.test(shannon_entropy~as.factor(Hclust),data=data.analysis) #Significant (p = 0.01)!
wilcox.test(log10(cfus_per_ml)~as.factor(Hclust),data=data.analysis) #Significant (p = 0.009)!
wilcox.test(PercentLarvFnd~as.factor(Hclust),data=data.analysis) #Significant (p = 0.01)!
wilcox.test(PercentTreat~as.factor(Hclust),data=data.analysis) #Significant (p = 0.02)!
wilcox.test(PercentRetreat~as.factor(Hclust),data=data.analysis) #Not significant (p = 0.2)!











data=read.csv("Combined-Master.csv",header=TRUE)
data=data[which(data$Year==2007|data$Year==2008|data$Year==2009|data$Year==2010|data$Year==2011|data$Year==2012|data$Year==2013|data$Year==2014|data$Year==2015|data$Year==2016|data$Year==2017|data$Year==2018),]
data=data[which(data$Posttreat==0),]
data=subset(data,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$LarvDens=sum.by.site$TotCnt/sum.by.site$NumDips
sum.by.site$LarvDens[is.nan(sum.by.site$LarvDens)]=0
data.new=subset(sum.by.site,select=c(PercLarvFnd,LarvDens))
rownames(data.new)=sum.by.site[,1]
data.new=t(data.new)
JSD=function(x,y)sqrt(0.5*KLD(x,(x+y)/2)+0.5*KLD(y,(x+y)/2))
KLD=function(x,y)sum(x*log(x/y))
dist.JSD=function(inMatrix,pseudocount=0.000001,...){
  KLD=function(x,y)sum(x*log(x/y))
  JSD=function(x,y)sqrt(0.5*KLD(x,(x+y)/2)+0.5*KLD(y,(x+y)/2))
  matrixColSize=length(colnames(inMatrix))
  matrixRowSize=length(rownames(inMatrix))
  colnames=colnames(inMatrix)
  resultsMatrix=matrix(0,matrixColSize,matrixColSize)
  inMatrix=apply(inMatrix,1:2,function(x)ifelse(x==0,pseudocount,x))
    for(i in 1:matrixColSize){
      for(j in 1:matrixColSize){
        resultsMatrix[i,j]=JSD(as.vector(inMatrix[,i]),
		as.vector(inMatrix[,j]))
		}
	  }
  colnames -> colnames(resultsMatrix) -> rownames(resultsMatrix)
  as.dist(resultsMatrix)->resultsMatrix
  attr(resultsMatrix,"method")="dist"
  return(resultsMatrix) 
  }
data.dist=dist.JSD(data.new)
pam.clustering=function(x,k) { # x is a distance matrix and k the number of clusters
                         require(cluster)
                         cluster = as.vector(pam(as.dist(x), k, diss=TRUE)$clustering)
                         return(cluster)
                        }
data.cluster=pam.clustering(data.dist,k=3)
obs.silhouette=mean(silhouette(data.cluster,data.dist)[,3])
obs.silhouette
obs.pca=dudi.pca(data.frame(t(data.new)), scannf=F, nf=10)
obs.bet=bca(obs.pca,fac=as.factor(data.cluster),scannf=F,nf=k-1) 
data.new=as.data.frame(t(data.new))
data.new$ProdClust=data.cluster
data.new$AreaNumb=rownames(data.new)
data=read.csv("Combined-Master.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate,Year))
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0 
data.total=merge(data.new,sum.by.site,by="AreaNumb")
data.total$PercRetreat=data.total$Retreat/data.total$SiteVis
data.total$PercRetreat2=data.total$Retreat/data.total$Posttreat
data.total$PercRetreat2[is.nan(data.total$PercRetreat2)]=0
data.total$Treat[data.total$Treat>0]=1
data.total$Retreat[data.total$Retreat>0]=1
data.alpha=read.csv("alpha-div.csv",header=TRUE)
data.analysis=merge(data.total,data.alpha,by="AreaNumb")
data.analysis$Hclust[data.analysis$Hclust==1]=0
data.analysis$Hclust[data.analysis$Hclust==2]=1
data.analysis$Pretreat=data.analysis$SiteVis-data.analysis$Posttreat
data.analysis$PercentSiteDry=(data.analysis$SiteDry/data.analysis$SiteVis)*100
data.analysis$PercentLarvFnd=data.analysis$PercLarvFnd*100
data.analysis$PercTreat=data.analysis$Treat/data.analysis$SiteVis
data.analysis$PercentTreat=(data.analysis$Treat/data.analysis$SiteVis)*100
data.analysis$PercentRetreat=data.analysis$PercRetreat*100
data.analysis$PercentRetreat2=data.analysis$PercRetreat2*100

wilcox.test(log10(observed_features)~as.factor(Hclust),data=data.analysis) #Significant (p = 0.01)!
wilcox.test(log10(faith_pd)~as.factor(Hclust),data=data.analysis) #Significant (p = 0.03)!
wilcox.test(pielou_evenness~as.factor(Hclust),data=data.analysis) #Significant (p = 0.04)!
wilcox.test(shannon_entropy~as.factor(Hclust),data=data.analysis) #Significant (p = 0.01)!
wilcox.test(log10(cfus_per_ml)~as.factor(Hclust),data=data.analysis) #Significant (p = 0.009)!
wilcox.test(PercentLarvFnd~as.factor(Hclust),data=data.analysis) #Significant (p = 0.01)!
wilcox.test(PercentTreat~as.factor(Hclust),data=data.analysis) #Significant (p = 0.02)!
wilcox.test(PercentRetreat~as.factor(Hclust),data=data.analysis) #Not significant (p = 0.2)!



wilcox.test(LarvDens~as.factor(Hclust),data=data.analysis) #Not significant (p = 0.2)!
wilcox.test(PercTreat~as.factor(Hclust),data=data.analysis) #Not significant (p = 0.2)!


fit<-glm(Hclust~PercentSiteDry,family="binomial",data=data.analysis)
summary(fit) #Not significant (p = 0.58549)!

fit<-glm(Hclust~PercentLarvFnd,family="binomial",data=data.analysis)
summary(fit) #Significant (p = 0.026)!
control=exp(-0.3995)/(1+exp(-0.3995))
100*(exp(-0.0431)/(1-control+(control*exp(-0.0431)))-1) #You are 2.57% less likely to be colonized by habitat microbiota biotype #2 for every 1% increase in PercentLarvFnd

fit<-glm(Hclust~LarvDens+PercentLarvFnd,family="binomial",data=data.analysis)
summary(fit) #Significant (p = 0.026)!
control=exp(-0.3995)/(1+exp(-0.3995))
100*(exp(-0.0431)/(1-control+(control*exp(-0.0431)))-1) #You are 2.57% less likely to be colonized by habitat microbiota biotype #2 for every 1% increase in PercentLarvFnd



fit<-glm(Hclust~Treat,family="binomial",data=data.analysis)
summary(fit) #Significant (p = 0.011)!
control=exp(-0.470)/(1+exp(-0.470))
100*(exp(-1.402)/(1-control+(control*exp(-1.402)))-1) #You are 65.3% less likely to be colonized by habitat microbiota biotype #2 if you've been treated at least once in the past

fit<-glm(Hclust~Treat,family="binomial",data=data.analysis)
summary(fit) #Significant (p = 0.011)!
control=exp(-0.470)/(1+exp(-0.470))
100*(exp(-1.402)/(1-control+(control*exp(-1.402)))-1) #You are 65.3% less likely to be colonized by habitat microbiota biotype #2 if you've been treated at least once in the past








fit<-glm(log10(observed_features)~PercentLarvFnd,family="gaussian",data=data.analysis)
summary(fit) #Not significant (p = 0.88)!

fit<-glm(log10(faith_pd)~PercentLarvFnd,family="gaussian",data=data.analysis)
summary(fit) #Not significant (p = 0.97)!

fit<-glm(pielou_evenness~PercentLarvFnd,family="gaussian",data=data.analysis)
summary(fit) #Not significant (p = 0.92)!

fit<-glm(shannon_entropy~PercentLarvFnd,family="gaussian",data=data.analysis)
summary(fit) #Not significant (p = 0.95)!

















data=read.csv("Combined-Master.csv",header=TRUE)
data=data[which(data$Posttreat==0),]
data=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
data.new=subset(sum.by.year,select=c(PercLarvFnd,LarvDens))
rownames(data.new)=paste(sum.by.year[,1],sum.by.year[,2],sep="_")
data.new=t(data.new)
hopkins=hopkins(data.new,n=nrow(data.new)-1)
hopkins
JSD=function(x,y)sqrt(0.5*KLD(x,(x+y)/2)+0.5*KLD(y,(x+y)/2))
KLD=function(x,y)sum(x*log(x/y))
dist.JSD=function(inMatrix,pseudocount=0.000001,...){
  KLD=function(x,y)sum(x*log(x/y))
  JSD=function(x,y)sqrt(0.5*KLD(x,(x+y)/2)+0.5*KLD(y,(x+y)/2))
  matrixColSize=length(colnames(inMatrix))
  matrixRowSize=length(rownames(inMatrix))
  colnames=colnames(inMatrix)
  resultsMatrix=matrix(0,matrixColSize,matrixColSize)
  inMatrix=apply(inMatrix,1:2,function(x)ifelse(x==0,pseudocount,x))
    for(i in 1:matrixColSize){
      for(j in 1:matrixColSize){
        resultsMatrix[i,j]=JSD(as.vector(inMatrix[,i]),
		as.vector(inMatrix[,j]))
		}
	  }
  colnames -> colnames(resultsMatrix) -> rownames(resultsMatrix)
  as.dist(resultsMatrix)->resultsMatrix
  attr(resultsMatrix,"method")="dist"
  return(resultsMatrix) 
  }
data.dist=dist.JSD(data.new)
pam.clustering=function(x,k) { # x is a distance matrix and k the number of clusters
                         require(cluster)
                         cluster = as.vector(pam(as.dist(x), k, diss=TRUE)$clustering)
                         return(cluster)
                        }
data.cluster=pam.clustering(data.dist,k=3)
obs.silhouette=mean(silhouette(data.cluster,data.dist)[,3])
obs.silhouette
obs.pca=dudi.pca(data.frame(t(data.new)), scannf=F, nf=10)
obs.bet=bca(obs.pca,fac=as.factor(data.cluster),scannf=F,nf=k-1) 
data.new=as.data.frame(t(data.new))
data.new$ProdClust=data.cluster
data.new$id=rownames(data.new)
data=read.csv("Combined-Master.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0 
sum.by.year$id=paste(sum.by.year$AreaNumb,sum.by.year$Year,sep="_")
data.total=merge(data.new,sum.by.year,by="id")
data.total$PercRetreat=data.total$Retreat/data.total$SiteVis
data.total$PercRetreat2=data.total$Retreat/data.total$Posttreat
data.total$PercRetreat2[is.nan(data.total$PercRetreat2)]=0
data.total$Treat[data.total$Treat>0]=1
data.total$Retreat[data.total$Retreat>0]=1
data.alpha=read.csv("alpha-div.csv",header=TRUE)
data.analysis=merge(data.total,data.alpha,by="AreaNumb")
data.analysis$Wave=data.analysis$Year
data.analysis$Wave[data.analysis$Wave==2007]=1
data.analysis$Wave[data.analysis$Wave==2008]=2
data.analysis$Wave[data.analysis$Wave==2009]=3
data.analysis$Wave[data.analysis$Wave==2010]=4
data.analysis$Wave[data.analysis$Wave==2011]=5
data.analysis$Wave[data.analysis$Wave==2012]=6
data.analysis$Wave[data.analysis$Wave==2013]=7
data.analysis$Wave[data.analysis$Wave==2014]=8
data.analysis$Wave[data.analysis$Wave==2015]=9
data.analysis$Wave[data.analysis$Wave==2016]=10
data.analysis$Wave[data.analysis$Wave==2017]=11
data.analysis$Wave[data.analysis$Wave==2018]=12
data.analysis$Wave[data.analysis$Wave==2019]=12
data.analysis$Wave[data.analysis$Wave==2020]=12
data.analysis$Wave[data.analysis$Wave==2021]=12
data.analysis=data.analysis[order(data.analysis$AreaNumb,data.analysis$Wave),]
data.analysis$Hclust[data.analysis$Hclust==1]=0
data.analysis$Hclust[data.analysis$Hclust==2]=1
data.analysis$Pretreat=data.analysis$SiteVis-data.analysis$Posttreat

library(geepack)
library(multgee)
library(car)
library(MESS)

data.hist=data.analysis[which(data.analysis$Year==2007|data.analysis$Year==2008|data.analysis$Year==2009|data.analysis$Year==2010|data.analysis$Year==2011|data.analysis$Year==2012|data.analysis$Year==2013|data.analysis$Year==2014|data.analysis$Year==2015|data.analysis$Year==2016|data.analysis$Year==2017|data.analysis$Year==2018),]
fit.1<-ordLORgee(ProdClust~as.factor(Hclust),id=AreaNumb,repeated=Wave,data=data.hist)
summary(fit.1)



fit.1<-geeglm(ProdClust~Hclust*Wave,id=AreaNumb,family=binomial,waves=Wave,corstr="independence",data=data.hist,weights=Pretreat)
summary(fit.1)
fit.2<-geeglm(ProdClust~Hclust*Wave,id=AreaNumb,family=binomial,waves=Wave,corstr="exchangeable",data=data.hist,weights=Pretreat)
summary(fit.2)
fit.3<-geeglm(ProdClust~Hclust*Wave,id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist,weights=Pretreat)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(ProdClust~Hclust*Wave,id=AreaNumb,family=binomial,waves=Wave,corstr="exchangeable",data=data.hist,weights=Pretreat)
summary(fit.2)





fit.1<-geeglm(Hclust~PercLarvFnd,id=AreaNumb,family=binomial,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(Hclust~PercLarvFnd,id=AreaNumb,family=binomial,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(Hclust~PercLarvFnd,id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(Hclust~PercLarvFnd,id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
control=exp(-1.33)/(1+exp(-1.33))
exp(1.77e-04)/(1-control+(control*exp(1.77e-04)))-1 #You are 33% less likely to find larvae in Cluster 1 sites than Cluster 0 sites prior to treatment










fit.1<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)

fit.1<-geeglm(Hclust~as.factor(Treat),id=AreaNumb,family=binomial,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(Hclust~as.factor(Treat),id=AreaNumb,family=binomial,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(Hclust~as.factor(Treat),id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(Hclust~as.factor(Treat),id=AreaNumb,family=binomial,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)




fit.1<-geeglm(log10(observed_features)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(log10(observed_features)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(log10(observed_features)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(log10(observed_features)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)

fit.1<-geeglm(log10(faith_pd)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(log10(faith_pd)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(log10(faith_pd)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(log10(faith_pd)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)

fit.1<-geeglm(shannon_entropy~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(shannon_entropy~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(shannon_entropy~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(shannon_entropy~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)

fit.1<-geeglm(pielou_evenness~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(pielou_evenness~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(pielou_evenness~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(pielou_evenness~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)

fit.1<-geeglm(log10(cfus_per_ml)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="independence",data=data.hist)
summary(fit.1)
fit.2<-geeglm(log10(cfus_per_ml)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)
fit.3<-geeglm(log10(cfus_per_ml)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="ar1",data=data.hist)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(log10(cfus_per_ml)~as.factor(ProdClust),id=AreaNumb,family=gaussian,waves=Wave,corstr="exchangeable",data=data.hist)
summary(fit.2)






fit.1<-geeglm(LarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.analysis)
summary(fit.1)
fit.2<-geeglm(LarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.analysis)
summary(fit.2)
fit.3<-geeglm(LarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.analysis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(LarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.analysis)
summary(fit.2)



fit.1<-ordLORgee(ProdClust~as.factor(Hclust),id=AreaNumb,repeated="Wave",data=data.analysis)
summary(fit.1)



control=exp(-1.322)/(1+exp(-1.322))
exp(-0.437)/(1-control+(control*exp(-0.437)))-1 #You are 33% less likely to find larvae in Cluster 1 sites than Cluster 0 sites prior to treatment



fit.1<-geeglm(Hclust~as.factor(ProdClust)*Wave,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.analysis,scale.fix=TRUE)
summary(fit.1)
fit.2<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.analysis)
summary(fit.2)
fit.3<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.analysis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.analysis)
summary(fit.3) #Hclust is not significant (p = 0.88)!

fit.1=nomLORgee(ProdClust~as.factor(Hclust),id=AreaNumb,repeated=Wave,LORstr="independence",data=data.analysis)
summary(fit.1)


fit.1<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.analysis,scale.fix=TRUE)
summary(fit.1)
fit.2<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.analysis)
summary(fit.2)
fit.3<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.analysis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(Hclust~as.factor(ProdClust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.analysis)
summary(fit.3) #Hclust is not significant (p = 0.88)!

fit.1<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2) #Hclust is significant (p = 0.032)!

fit.1<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2) #Hclust is significant (p = 0.032)!

fit.1<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2) #Hclust is significant (p = 0.032)!

data.new=data[!is.na(data$SiteVisPretreat|data$SiteDryPretreat|data$NumDipsPretreat|data$LarvFndPretreat|data$TotCntPretreat|data$AeCntPretreat|data$AnCntPretreat|data$CxCntPretreat|data$VecCntPretreat|data$NonVecCntPretreat|data$LarvDivPretreat),]

fit.1<-geeglm(PercSiteDryPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.new,weights=SiteVisPretreat)
summary(fit.1)
fit.2<-geeglm(PercSiteDryPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new,weights=SiteVisPretreat)
summary(fit.2)
fit.3<-geeglm(PercSiteDryPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new,weights=SiteVisPretreat)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercSiteDryPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new,weights=SiteVisPretreat)
summary(fit.2) #Hclust is not significant (p = 0.44)!

fit.1<-geeglm(PercLarvFndPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.new,weights=SiteVisPretreat)
summary(fit.1)
fit.2<-geeglm(PercLarvFndPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new,weights=SiteVisPretreat)
summary(fit.2)
fit.3<-geeglm(PercLarvFndPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new,weights=SiteVisPretreat)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFndPretreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new,weights=SiteVisPretreat)
summary(fit.2) #Hclust is significant (p = 0.044)!
control=exp(-1.322)/(1+exp(-1.322))
exp(-0.437)/(1-control+(control*exp(-0.437)))-1 #You are 33% less likely to find larvae in Cluster 1 sites than Cluster 0 sites prior to treatment

fit.1<-geeglm(LarvDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data.new,offset=log(SiteVisPretreat))
summary(fit.1)
fit.2<-geeglm(LarvDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2)
fit.3<-geeglm(LarvDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(LarvDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2) #HClust not significant (p = 0.11)!

fit.1<-geeglm(AeDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data.new,offset=log(SiteVisPretreat))
summary(fit.1)
fit.2<-geeglm(AeDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2)
fit.3<-geeglm(AeDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(AeDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3) #HClust not significant (p = 0.34)!

fit.1<-geeglm(CxDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data.new,offset=log(SiteVisPretreat))
summary(fit.1)
fit.2<-geeglm(CxDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2)
fit.3<-geeglm(CxDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(CxDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2) #HClust significant (p = 0.041)!
exp(0.72773)-1 #Being in Cluster 1 is associated with a 107% increase in Culex spp. larval density prior to treatment
exp(0.03773)-1 #Culex spp. larval density per site visit increases by 3.85% for every 1% increase in the frequency at which larvae are detected in an individual site prior to treatment

fit.1<-geeglm(NonVecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data.new,offset=log(SiteVisPretreat))
summary(fit.1)
fit.2<-geeglm(NonVecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2)
fit.3<-geeglm(NonVecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(NonVecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3) #Hclust is not significant (p = 0.34)!

fit.1<-geeglm(VecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data.new,offset=log(SiteVisPretreat))
summary(fit.1)
fit.2<-geeglm(VecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2)
fit.3<-geeglm(VecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVisPretreat))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(VecDensPretreat~as.factor(Hclust)+PercentLarvFndPretreat,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVisPretreat))
summary(fit.2) #HClust is significant (p = 0.047)!
exp(0.73333)-1 #Being in Cluster 1 is associated with a 108% increase in Culex vector spp. larval density prior to treatment
exp(0.03805)-1 #Culex vector larval density per site visit increases by 3.88% for every 1% increase in the frequency at which larvae are detected in an individual site prior to treatment

data.new2=data[!is.na(data$SiteVisPosttreat|data$SiteDryPosttreat|data$NumDipsPosttreat|data$LarvFndPosttreat|data$TotCntPosttreat|data$AeCntPosttreat|data$AnCntPosttreat|data$CxCntPosttreat|data$VecCntPosttreat|data$NonVecCntPosttreat|data$LarvDivPosttreat),]

fit.1<-geeglm(PercSiteDryPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.new2,weights=SiteVisPosttreat)
summary(fit.1)
fit.2<-geeglm(PercSiteDryPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new2,weights=SiteVisPosttreat)
summary(fit.2)
fit.3<-geeglm(PercSiteDryPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new2,weights=SiteVisPosttreat)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercSiteDryPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new2,weights=SiteVisPosttreat)
summary(fit.2) #Hclust is not significant (p = 0.21)!

fit.1<-geeglm(PercLarvFndPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.new2,weights=SiteVisPosttreat)
summary(fit.1)
fit.2<-geeglm(PercLarvFndPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new2,weights=SiteVisPosttreat)
summary(fit.2)
fit.3<-geeglm(PercLarvFndPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new2,weights=SiteVisPosttreat)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFndPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new2,weights=SiteVisPosttreat)
summary(fit.2) #Hclust is not significant (p = 0.48290)!

fit.1<-geeglm(PercRetreatPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.new2,weights=SiteVisPosttreat)
summary(fit.1)
fit.2<-geeglm(PercRetreatPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new2,weights=SiteVisPosttreat)
summary(fit.2)
fit.3<-geeglm(PercRetreatPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new2,weights=SiteVisPosttreat)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercRetreatPosttreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new2,weights=SiteVisPosttreat)
summary(fit.2) #Hclust is not significant (p = 0.94)!

Sum=Summarize(PercSiteDry~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.25))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercLarvFnd~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.3))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercTreat~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.1))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$PostTreat!=1)
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
write.csv(sum.by.year,"temp.csv", row.names = FALSE)

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$PostTreat!=0)
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
write.csv(sum.by.year,"temp.csv", row.names = FALSE)

data=read.csv("Curr-Master-Final.csv",header=TRUE)
data=data[which(data$AreaNumb==4|data$AreaNumb==5|data$AreaNumb==6|data$AreaNumb==13|data$AreaNumb==26|data$AreaNumb==81|data$AreaNumb==83|data$AreaNumb==86|data$AreaNumb==88|data$AreaNumb==158|data$AreaNumb==164|data$AreaNumb==189|data$AreaNumb==218|data$AreaNumb==243|data$AreaNumb==245|data$AreaNumb==247|data$AreaNumb==249|data$AreaNumb==251|data$AreaNumb==253|data$AreaNumb==254|data$AreaNumb==256|data$AreaNumb==319|data$AreaNumb==335|data$AreaNumb==340|data$AreaNumb==369|data$AreaNumb==371|data$AreaNumb==584|data$AreaNumb==391|data$AreaNumb==397|data$AreaNumb==417|data$AreaNumb==465|data$AreaNumb==513|data$AreaNumb==515|data$AreaNumb==522|data$AreaNumb==523|data$AreaNumb==526|data$AreaNumb==530|data$AreaNumb==552|data$AreaNumb==556|data$AreaNumb==558|data$AreaNumb==559|data$AreaNumb==565|data$AreaNumb==583|data$AreaNumb==587|data$AreaNumb==590|data$AreaNumb==597|data$AreaNumb==607|data$AreaNumb==615|data$AreaNumb==618|data$AreaNumb==619|data$AreaNumb==620|data$AreaNumb==676|data$AreaNumb==700|data$AreaNumb==715|data$AreaNumb==719|data$AreaNumb==724|data$AreaNumb==725|data$AreaNumb==900|data$AreaNumb==969|data$AreaNumb==972|data$AreaNumb==973|data$AreaNumb==975|data$AreaNumb==978|data$AreaNumb==1931|data$AreaNumb==1932|data$AreaNumb==2041|data$AreaNumb==3262|data$AreaNumb==3333|data$AreaNumb==3921|data$AreaNumb==3922|data$AreaNumb==4022|data$AreaNumb==4130|data$AreaNumb==8300|data$AreaNumb==8301|data$AreaNumb==8512|data$AreaNumb==9007|data$AreaNumb==9009|data$AreaNumb==9014|data$AreaNumb==9019|data$AreaNumb==9098|data$AreaNumb==9601|data$AreaNumb==9970|data$AreaNumb==9971),]
write.csv(data,"temp.csv", row.names = FALSE)

data=read.csv("Curr-Master-Final-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercSiteDry=sum.by.year$SiteDry/sum.by.year$SiteVis
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$AeDens=sum.by.year$AeCnt/sum.by.year$NumDips
sum.by.year$AnDens=sum.by.year$AnCnt/sum.by.year$NumDips
sum.by.year$CxDens=sum.by.year$CxCnt/sum.by.year$NumDips
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercRetreat=sum.by.year$Retreat/sum.by.year$SiteVis
sum.by.year$PercSiteDry[is.nan(sum.by.year$PercSiteDry)]=0
sum.by.year$PercLarvFnd[is.nan(sum.by.year$PercLarvFnd)]=0
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
sum.by.year$AeDens[is.nan(sum.by.year$AeDens)]=0
sum.by.year$AnDens[is.nan(sum.by.year$AnDens)]=0
sum.by.year$CxDens[is.nan(sum.by.year$CxDens)]=0
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
sum.by.year$PercRetreat[is.nan(sum.by.year$PercRetreat)]=0
final=sum.by.year[which(sum.by.year$AreaNumb==4|sum.by.year$AreaNumb==5|sum.by.year$AreaNumb==6|sum.by.year$AreaNumb==13|sum.by.year$AreaNumb==26|sum.by.year$AreaNumb==81|sum.by.year$AreaNumb==83|sum.by.year$AreaNumb==86|sum.by.year$AreaNumb==88|sum.by.year$AreaNumb==158|sum.by.year$AreaNumb==164|sum.by.year$AreaNumb==189|sum.by.year$AreaNumb==218|sum.by.year$AreaNumb==243|sum.by.year$AreaNumb==245|sum.by.year$AreaNumb==247|sum.by.year$AreaNumb==249|sum.by.year$AreaNumb==251|sum.by.year$AreaNumb==253|sum.by.year$AreaNumb==254|sum.by.year$AreaNumb==256|sum.by.year$AreaNumb==319|sum.by.year$AreaNumb==335|sum.by.year$AreaNumb==340|sum.by.year$AreaNumb==369|sum.by.year$AreaNumb==371|sum.by.year$AreaNumb==584|sum.by.year$AreaNumb==391|sum.by.year$AreaNumb==397|sum.by.year$AreaNumb==417|sum.by.year$AreaNumb==465|sum.by.year$AreaNumb==513|sum.by.year$AreaNumb==515|sum.by.year$AreaNumb==522|sum.by.year$AreaNumb==523|sum.by.year$AreaNumb==526|sum.by.year$AreaNumb==530|sum.by.year$AreaNumb==552|sum.by.year$AreaNumb==556|sum.by.year$AreaNumb==558|sum.by.year$AreaNumb==559|sum.by.year$AreaNumb==565|sum.by.year$AreaNumb==583|sum.by.year$AreaNumb==587|sum.by.year$AreaNumb==590|sum.by.year$AreaNumb==597|sum.by.year$AreaNumb==607|sum.by.year$AreaNumb==615|sum.by.year$AreaNumb==618|sum.by.year$AreaNumb==619|sum.by.year$AreaNumb==620|sum.by.year$AreaNumb==676|sum.by.year$AreaNumb==700|sum.by.year$AreaNumb==715|sum.by.year$AreaNumb==719|sum.by.year$AreaNumb==724|sum.by.year$AreaNumb==725|sum.by.year$AreaNumb==900|sum.by.year$AreaNumb==969|sum.by.year$AreaNumb==972|sum.by.year$AreaNumb==973|sum.by.year$AreaNumb==975|sum.by.year$AreaNumb==978|sum.by.year$AreaNumb==1931|sum.by.year$AreaNumb==1932|sum.by.year$AreaNumb==2041|sum.by.year$AreaNumb==3262|sum.by.year$AreaNumb==3333|sum.by.year$AreaNumb==3921|sum.by.year$AreaNumb==3922|sum.by.year$AreaNumb==4022|sum.by.year$AreaNumb==4130|sum.by.year$AreaNumb==8300|sum.by.year$AreaNumb==8301|sum.by.year$AreaNumb==8512|sum.by.year$AreaNumb==9007|sum.by.year$AreaNumb==9009|sum.by.year$AreaNumb==9014|sum.by.year$AreaNumb==9019|sum.by.year$AreaNumb==9098|sum.by.year$AreaNumb==9601|sum.by.year$AreaNumb==9970|sum.by.year$AreaNumb==9971),]
write.csv(final,"temp.csv", row.names = FALSE)

data=read.csv("Curr-Master-Final-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$PostTreat!=1)
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
write.csv(sum.by.year,"temp2.csv", row.names = FALSE)

data=read.csv("Curr-Master-Final-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$PostTreat!=0)
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
write.csv(sum.by.year,"temp2.csv", row.names = FALSE)

data=read.csv("Curr-MB-Comp-Collated-By-Site-Year.csv",header=TRUE)
data$NonVecCnt=data$TotCnt-data$VecCnt
data$NonVecCntPretreat=data$TotCntPretreat-data$VecCntPretreat
data$NonVecCntPosttreat=data$TotCntPosttreat-data$VecCntPosttreat
data$Wave=data$Year
data$Wave[data$Wave==2019]=1
data$Wave[data$Wave==2020]=2
data$Wave[data$Wave==2021]=3
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

data$Hclust[data$Hclust==1]=0
data$Hclust[data$Hclust==2]=1
data$LarvFndBin=ifelse(data$PercLarvFnd>0,1,0)
data$TreatBin=ifelse(data$PercTreat>0,1,0)
data$RetreatBin=ifelse(data$PercRetreat>0,1,0)
data$LarvDens[is.nan(data$LarvDens)]=0
data$LarvDens=round(data$LarvDens,0)
data$AeDens=data$AeCnt/data$NumDips
data$AeDens[is.nan(data$AeDens)]=0
data$AeDens=round(data$AeDens,0)
data$NonVecDens=data$NonVecCnt/data$NumDips
data$NonVecDens[is.nan(data$NonVecDens)]=0
data$NonVecDens=round(data$NonVecDens,0)
data$VecDens=data$VecCnt/data$NumDips
data$VecDens[is.nan(data$VecDens)]=0
data$VecDens=round(data$VecDens,0)

fit.1<-geeglm(PercLarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercLarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercLarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2) #Hclust is significant (p = 0.011)!

fit.1<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(LarvFndBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2) #Hclust is significant (p = 0.02)!

fit.1<-geeglm(PercTreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercTreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercTreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(PercTreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3) #Hclust is not significant (p = 0.083)!

fit.1<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(TreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2) #Hclust is significant (p = 0.01375)!

fit.1<-geeglm(PercRetreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercRetreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercRetreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(PercRetreat~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3) #Hclust is significant (p = 0.042)!

fit.1<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(RetreatBin~as.factor(Hclust),id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2) #Hclust is significant (p = 0.04)!































Sum=Summarize(PercSiteDry~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.2))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercLarvFnd~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.4))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercTreat~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.1))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

data=read.csv("Hist-LarvalData.csv",header=TRUE)
data=data[which(data$AreaNumb==4|data$AreaNumb==5|data$AreaNumb==6|data$AreaNumb==13|data$AreaNumb==26|data$AreaNumb==81|data$AreaNumb==83|data$AreaNumb==86|data$AreaNumb==88|data$AreaNumb==158|data$AreaNumb==164|data$AreaNumb==189|data$AreaNumb==218|data$AreaNumb==243|data$AreaNumb==245|data$AreaNumb==247|data$AreaNumb==249|data$AreaNumb==251|data$AreaNumb==253|data$AreaNumb==254|data$AreaNumb==256|data$AreaNumb==319|data$AreaNumb==335|data$AreaNumb==340|data$AreaNumb==369|data$AreaNumb==371|data$AreaNumb==584|data$AreaNumb==391|data$AreaNumb==397|data$AreaNumb==417|data$AreaNumb==465|data$AreaNumb==513|data$AreaNumb==515|data$AreaNumb==522|data$AreaNumb==523|data$AreaNumb==526|data$AreaNumb==530|data$AreaNumb==552|data$AreaNumb==556|data$AreaNumb==558|data$AreaNumb==559|data$AreaNumb==565|data$AreaNumb==583|data$AreaNumb==587|data$AreaNumb==590|data$AreaNumb==597|data$AreaNumb==607|data$AreaNumb==615|data$AreaNumb==618|data$AreaNumb==619|data$AreaNumb==620|data$AreaNumb==676|data$AreaNumb==700|data$AreaNumb==715|data$AreaNumb==719|data$AreaNumb==724|data$AreaNumb==725|data$AreaNumb==900|data$AreaNumb==969|data$AreaNumb==972|data$AreaNumb==973|data$AreaNumb==975|data$AreaNumb==978|data$AreaNumb==1931|data$AreaNumb==1932|data$AreaNumb==2041|data$AreaNumb==3262|data$AreaNumb==3333|data$AreaNumb==3921|data$AreaNumb==3922|data$AreaNumb==4022|data$AreaNumb==4130|data$AreaNumb==8300|data$AreaNumb==8301|data$AreaNumb==8512|data$AreaNumb==9007|data$AreaNumb==9009|data$AreaNumb==9014|data$AreaNumb==9019|data$AreaNumb==9098|data$AreaNumb==9601|data$AreaNumb==9970|data$AreaNumb==9971),]
write.csv(data,"temp.csv", row.names = FALSE)

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercRetreat=sum.by.year$Retreat/sum.by.year$SiteVis
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
sum.by.year$PercRetreat[is.nan(sum.by.year$PercRetreat)]=0
write.csv(sum.by.year,"temp.csv", row.names = FALSE)

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$Retreat!=1)
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
write.csv(sum.by.year,"temp.csv", row.names = FALSE)

data=read.csv("Hist-LarvalData-Collated-Counts.csv",header=TRUE)
data$Wave=data$Year
data$Wave[data$Wave==2007]=1
data$Wave[data$Wave==2008]=2
data$Wave[data$Wave==2009]=3
data$Wave[data$Wave==2010]=4
data$Wave[data$Wave==2011]=5
data$Wave[data$Wave==2012]=6
data$Wave[data$Wave==2013]=7
data$Wave[data$Wave==2014]=8
data$Wave[data$Wave==2015]=9
data$Wave[data$Wave==2016]=10
data$Wave[data$Wave==2017]=11
data$Wave[data$Wave==2018]=12
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

data$Hclust[data$Hclust==1]=0
data$Hclust[data$Hclust==2]=1

fit.1<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(TotCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

fit.1<-geeglm(AeCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(AeCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(AeCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(AeCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

fit.1<-geeglm(AnCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(AnCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(AnCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(AnCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

Sum=Summarize((AnCnt/NumDips)~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,.1))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

fit.1<-geeglm(CxCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(CxCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(CxCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(CxCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

Sum=Summarize((CxCnt/NumDips)~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,9))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

fit.1<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

Sum=Summarize((VecCnt/NumDips)~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,9))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

fit.1<-geeglm(NonVecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(NonVecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(NonVecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(NonVecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$Retreat!=0)
data=subset(data,data$SiteDry!=1)
sum.by.year=aggregate(.~AreaNumb+Year,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
write.csv(sum.by.year,"temp.csv", row.names = FALSE)

data=read.csv("Hist-LarvalData-Collated-Counts2.csv",header=TRUE)
data$Wave=data$Year
data$Wave[data$Wave==2007]=1
data$Wave[data$Wave==2008]=2
data$Wave[data$Wave==2009]=3
data$Wave[data$Wave==2010]=4
data$Wave[data$Wave==2011]=5
data$Wave[data$Wave==2012]=6
data$Wave[data$Wave==2013]=7
data$Wave[data$Wave==2014]=8
data$Wave[data$Wave==2015]=9
data$Wave[data$Wave==2016]=10
data$Wave[data$Wave==2017]=11
data$Wave[data$Wave==2018]=12
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

data$Hclust[data$Hclust==1]=0
data$Hclust[data$Hclust==2]=1

fit.1<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(NumDips+1))
summary(fit.1)
fit.2<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
fit.3<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(NumDips+1))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(VecCnt~as.factor(Hclust),id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(NumDips+1))
summary(fit.2)
QIC(fit.2)

Sum=Summarize((VecCnt/NumDips)~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,20))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$Retreat!=1)
data=subset(data,data$AreaNumb!=164)
data=subset(data,data$AreaNumb!=2041)
data=subset(data,data$AreaNumb!=218)
data=subset(data,data$AreaNumb!=249)
data=subset(data,data$AreaNumb!=3262)
data=subset(data,data$AreaNumb!=397)
data=subset(data,data$AreaNumb!=719)
data=subset(data,data$AreaNumb!=724)
data=subset(data,data$AreaNumb!=8300)
data=subset(data,data$AreaNumb!=969)
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
write.csv(sum.by.site,"temp.csv", row.names = FALSE)

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate))
data=subset(data,data$AreaNumb!=164)
data=subset(data,data$AreaNumb!=2041)
data=subset(data,data$AreaNumb!=218)
data=subset(data,data$AreaNumb!=249)
data=subset(data,data$AreaNumb!=3262)
data=subset(data,data$AreaNumb!=397)
data=subset(data,data$AreaNumb!=719)
data=subset(data,data$AreaNumb!=724)
data=subset(data,data$AreaNumb!=8300)
data=subset(data,data$AreaNumb!=969)
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0
write.csv(sum.by.site,"temp2.csv", row.names = FALSE)

data=read.csv("temp2.csv",header=TRUE)
data$PercLarvFnd=data$LarvFnd/data$SiteVis
data$LarvDens=data$TotCnt/data$NumDips
data$PercLarvFnd[is.nan(data$PercLarvFnd)]=0
data$LarvDens[is.nan(data$LarvDens)]=0
data$PercLarvfnd.z=(data$PercLarvFnd-mean(data$PercLarvFnd))/sd(data$PercLarvFnd)
data$LarvDens.z=(data$LarvDens-mean(data$LarvDens))/sd(data$LarvDens)
data.only=subset(data,select=c(PercLarvfnd.z,LarvDens.z))
data.dist=dist(data.only,method="euclidean",diag=FALSE)

library(mvabund)
library(vegan)

clust.single=hclust(data.dist,method='single')
clust.complete=hclust(data.dist,method='complete')
clust.UPGMA=hclust(data.dist,method='average')
clust.ward=hclust(data.dist,method='ward.D')

# cophenetic distance
clust.single.coph=cophenetic(clust.single)
clust.complete.coph=cophenetic(clust.complete)
clust.UPGMA.coph=cophenetic(clust.UPGMA)
clust.ward.coph=cophenetic(clust.ward)

# plot
par(mfrow=c(2,2))

# single
plot(data.dist,clust.single.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Single',paste('Cophenetic correlation',round(cor(data.dist,clust.single.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.single.coph),col='red')

# complete
plot(data.dist,clust.complete.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Complete',paste('Cophenetic correlation',round(cor(data.dist,clust.complete.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.complete.coph),col='red')

# UPGMA
plot(data.dist,clust.UPGMA.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('UPGMA',paste('Cophenetic correlation',round(cor(data.dist,clust.UPGMA.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.UPGMA.coph),col='red')

# ward
plot(data.dist,clust.ward.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Ward',paste('Cophenetic correlation',round(cor(data.dist,clust.ward.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.ward.coph),col='red')

# Looking for interpretable clusters
library(factoextra)
library(cluster)
fviz_nbclust(as.matrix(data.dist),hcut,method='silhouette',hc_method='average')
fviz_nbclust(as.matrix(data.dist),hcut,method='wss',hc_method='average')
cutg=cutree(clust.UPGMA,k=2)
sil=silhouette(cutg,data.dist)
plot(sil)
plot(clust.UPGMA)

data=read.csv("Hist-LarvalData-Collated.csv",header=TRUE)
data=subset(data,select=-c(DipID,SampDate,Year))
data=subset(data,data$AreaNumb!=164)
data=subset(data,data$AreaNumb!=2041)
data=subset(data,data$AreaNumb!=218)
data=subset(data,data$AreaNumb!=249)
data=subset(data,data$AreaNumb!=3262)
data=subset(data,data$AreaNumb!=397)
data=subset(data,data$AreaNumb!=719)
data=subset(data,data$AreaNumb!=724)
data=subset(data,data$AreaNumb!=8300)
data=subset(data,data$AreaNumb!=969)
sum.by.site=aggregate(.~AreaNumb,data,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0
sum.by.site$PercTreat=sum.by.site$Treat/sum.by.site$SiteVis
sum.by.site$PercRetreat=sum.by.site$Retreat/sum.by.site$SiteVis
sum.by.site$PercTreat[is.nan(sum.by.site$PercTreat)]=0
sum.by.site$PercRetreat[is.nan(sum.by.site$PercRetreat)]=0
data.only=subset(sum.by.site,select=c(PercTreat,PercRetreat))
data.dist=dist(data.only,method="euclidean",diag=FALSE)

library(mvabund)
library(vegan)

clust.single=hclust(data.dist,method='single')
clust.complete=hclust(data.dist,method='complete')
clust.UPGMA=hclust(data.dist,method='average')
clust.ward=hclust(data.dist,method='ward.D')

# cophenetic distance
clust.single.coph=cophenetic(clust.single)
clust.complete.coph=cophenetic(clust.complete)
clust.UPGMA.coph=cophenetic(clust.UPGMA)
clust.ward.coph=cophenetic(clust.ward)

# plot
par(mfrow=c(2,2))

# single
plot(data.dist,clust.single.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Single',paste('Cophenetic correlation',round(cor(data.dist,clust.single.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.single.coph),col='red')

# complete
plot(data.dist,clust.complete.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Complete',paste('Cophenetic correlation',round(cor(data.dist,clust.complete.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.complete.coph),col='red')

# UPGMA
plot(data.dist,clust.UPGMA.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('UPGMA',paste('Cophenetic correlation',round(cor(data.dist,clust.UPGMA.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.UPGMA.coph),col='red')

# ward
plot(data.dist,clust.ward.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Ward',paste('Cophenetic correlation',round(cor(data.dist,clust.ward.coph),3))))
abline(0,1)
lines(lowess(data.dist,clust.ward.coph),col='red')

# Looking for interpretable clusters
library(factoextra)
library(cluster)
fviz_nbclust(as.matrix(data.dist),hcut,method='silhouette',hc_method='average')
fviz_nbclust(as.matrix(data.dist),hcut,method='wss',hc_method='average')

grpdist=function(x){
	require(cluster)
	gr=as.data.frame(as.factor(x))
	distgr=daisy(gr,'gower')
	distgr
	}
kt=data.frame(k=1:nrow(data.only),r=0)
for(i in 2:(nrow(data.only)-1)){
	gr=cutree(clust.UPGMA,i)
	distgr=grpdist(gr)
	mt=cor(data.dist,distgr,method='pearson')
	kt[i,2]=mt
	}
k.best=which.max(kt$r)
plot(kt$k,kt$r,
	type='h',main='Mantel-optimal number of clusters-UPGMA',
	xlab='k (number of groups)',ylab="Pearson's correlation")
axis(1,k.best,
	paste('optimum',k.best,sep='\n'),col='red',font=2,col.axis='red')
points(k.best,max(kt$r),pch=16,col='red',cex=1.5)

cutg=cutree(clust.UPGMA,k=2)
sil=silhouette(cutg,data.dist)
plot(sil)
plot(clust.UPGMA)











fit.1<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)

fit.1<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)

fit.1<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,weights=NumDips)
summary(fit.1)
fit.2<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,weights=NumDips)
summary(fit.2)
fit.3<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,weights=NumDips)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.1<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,weights=NumDips)
summary(fit.1)

Sum=Summarize(PercSiteDry~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.3))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercLarvFnd~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.35))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercTreat~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.125))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(LarvDens~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,6))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(CxDens~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,4.5))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

data=read.csv("Curr-MB-Comp-Collated-By-Site-Year.csv",header=TRUE)
data=data[which(data$Year==2019),]
data=data[order(data$AreaNumb),]

library(geepack)
library(car)
library(MESS)

data$Hclust[data$Hclust==1]=0
data$Hclust[data$Hclust==2]=1

fit<-geeglm(PercSiteDry~Hclust,id=AreaNumb,family=binomial,data=data,weights=SiteVis)
summary(fit)

fit<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,data=data,weights=SiteVis)
summary(fit)

fit<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,data=data,weights=SiteVis)
summary(fit)

fit<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,data=data,weights=SiteVis)
summary(fit)


Sum=Summarize(PercSiteDry~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.3))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercLarvFnd~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.4))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercTreat~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.1))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(LarvDens~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,4))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(CxDens~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,4.5))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))


data=read.csv("Curr-MB-Comp-Collated-By-Site-Year.csv",header=TRUE)
data=data[which(data$Year==2019|data$Year==2020),]
data$Wave=data$Year
data$Wave[data$Wave==2019]=1
data$Wave[data$Wave==2020]=2
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

data$Hclust[data$Hclust==1]=0
data$Hclust[data$Hclust==2]=1

fit.1<-geeglm(PercSiteDry~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercSiteDry~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercSiteDry~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercSiteDry~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)

fit.1<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)

fit.1<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercTreat~Hclust,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)

fit.1<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,weights=NumDips)
summary(fit.1)
fit.2<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,weights=NumDips)
summary(fit.2)
fit.3<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,weights=NumDips)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.1<-geeglm(TotCnt~Hclust,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,weights=NumDips)
summary(fit.1)

Sum=Summarize(PercSiteDry~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.3))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercLarvFnd~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.35))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(PercTreat~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,0.125))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(LarvDens~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,6))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(CxDens~Hclust,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Hclust,y=mean,color=Hclust,data=Sum,ylim=c(0,4.5))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))































data=read.csv("Hist-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,Year,SampDate))
sum.by.site=aggregate(.~AreaNumb,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$LarvDens=sum.by.site$TotCnt/sum.by.site$NumDips
sum.by.site$AeDens=sum.by.site$AeCnt/sum.by.site$NumDips
sum.by.site$AnDens=sum.by.site$AnCnt/sum.by.site$NumDips
sum.by.site$CxDens=sum.by.site$CxCnt/sum.by.site$NumDips
sum.by.site$PercTreat=sum.by.site$Treat/sum.by.site$SiteVis
sum.by.site$PercSiteDry[is.nan(sum.by.site$PercSiteDry)]=0
sum.by.site$PercLarvFnd[is.nan(sum.by.site$PercLarvFnd)]=0
sum.by.site$LarvDens[is.nan(sum.by.site$LarvDens)]=0
sum.by.site$AeDens[is.nan(sum.by.site$AeDens)]=0
sum.by.site$AnDens[is.nan(sum.by.site$AnDens)]=0
sum.by.site$CxDens[is.nan(sum.by.site$CxDens)]=0
sum.by.site$PercTreat[is.nan(sum.by.site$PercTreat)]=0
final=sum.by.site[which(sum.by.site$AreaNumb==4|sum.by.site$AreaNumb==5|sum.by.site$AreaNumb==6|sum.by.site$AreaNumb==13|sum.by.site$AreaNumb==26|sum.by.site$AreaNumb==81|sum.by.site$AreaNumb==83|sum.by.site$AreaNumb==86|sum.by.site$AreaNumb==88|sum.by.site$AreaNumb==158|sum.by.site$AreaNumb==164|sum.by.site$AreaNumb==189|sum.by.site$AreaNumb==218|sum.by.site$AreaNumb==243|sum.by.site$AreaNumb==245|sum.by.site$AreaNumb==247|sum.by.site$AreaNumb==249|sum.by.site$AreaNumb==251|sum.by.site$AreaNumb==253|sum.by.site$AreaNumb==254|sum.by.site$AreaNumb==256|sum.by.site$AreaNumb==319|sum.by.site$AreaNumb==335|sum.by.site$AreaNumb==340|sum.by.site$AreaNumb==369|sum.by.site$AreaNumb==371|sum.by.site$AreaNumb==584|sum.by.site$AreaNumb==391|sum.by.site$AreaNumb==397|sum.by.site$AreaNumb==417|sum.by.site$AreaNumb==465|sum.by.site$AreaNumb==513|sum.by.site$AreaNumb==515|sum.by.site$AreaNumb==522|sum.by.site$AreaNumb==523|sum.by.site$AreaNumb==526|sum.by.site$AreaNumb==530|sum.by.site$AreaNumb==552|sum.by.site$AreaNumb==556|sum.by.site$AreaNumb==558|sum.by.site$AreaNumb==559|sum.by.site$AreaNumb==565|sum.by.site$AreaNumb==583|sum.by.site$AreaNumb==587|sum.by.site$AreaNumb==590|sum.by.site$AreaNumb==597|sum.by.site$AreaNumb==607|sum.by.site$AreaNumb==615|sum.by.site$AreaNumb==618|sum.by.site$AreaNumb==619|sum.by.site$AreaNumb==620|sum.by.site$AreaNumb==676|sum.by.site$AreaNumb==700|sum.by.site$AreaNumb==715|sum.by.site$AreaNumb==719|sum.by.site$AreaNumb==724|sum.by.site$AreaNumb==725|sum.by.site$AreaNumb==900|sum.by.site$AreaNumb==969|sum.by.site$AreaNumb==972|sum.by.site$AreaNumb==973|sum.by.site$AreaNumb==975|sum.by.site$AreaNumb==978|sum.by.site$AreaNumb==1931|sum.by.site$AreaNumb==1932|sum.by.site$AreaNumb==2041|sum.by.site$AreaNumb==3262|sum.by.site$AreaNumb==3333|sum.by.site$AreaNumb==3921|sum.by.site$AreaNumb==3922|sum.by.site$AreaNumb==4022|sum.by.site$AreaNumb==4130|sum.by.site$AreaNumb==8300|sum.by.site$AreaNumb==8301|sum.by.site$AreaNumb==8512|sum.by.site$AreaNumb==9007|sum.by.site$AreaNumb==9009|sum.by.site$AreaNumb==9014|sum.by.site$AreaNumb==9019|sum.by.site$AreaNumb==9098|sum.by.site$AreaNumb==9601|sum.by.site$AreaNumb==9970|sum.by.site$AreaNumb==9971),]
write.csv(final,"temp.csv", row.names = FALSE)

qiime tools export \
  --input-path core-metrics-results/observed_features_vector.qza \
  --output-path exported

qiime tools export \
  --input-path core-metrics-results/faith_pd_vector.qza \
  --output-path exported

qiime tools export \
  --input-path core-metrics-results/evenness_vector.qza \
  --output-path exported

qiime tools export \
  --input-path core-metrics-results/shannon_vector.qza \
  --output-path exported

qiime diversity alpha-rarefaction \
  --i-table final-filtered-table-98.qza \
  --p-max-depth 5000 \
  --p-metrics 'observed_features' \
  --o-visualization alpha-rarefaction.qzv

data=read.csv("Hist-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercSiteDry=sum.by.year$SiteDry/sum.by.year$SiteVis
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$AeDens=sum.by.year$AeCnt/sum.by.year$NumDips
sum.by.year$AnDens=sum.by.year$AnCnt/sum.by.year$NumDips
sum.by.year$CxDens=sum.by.year$CxCnt/sum.by.year$NumDips
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercSiteDry[is.nan(sum.by.year$PercSiteDry)]=0
sum.by.year$PercLarvFnd[is.nan(sum.by.year$PercLarvFnd)]=0
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
sum.by.year$AeDens[is.nan(sum.by.year$AeDens)]=0
sum.by.year$AnDens[is.nan(sum.by.year$AnDens)]=0
sum.by.year$CxDens[is.nan(sum.by.year$CxDens)]=0
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
final=sum.by.year[which(sum.by.year$AreaNumb==4|sum.by.year$AreaNumb==5|sum.by.year$AreaNumb==6|sum.by.year$AreaNumb==13|sum.by.year$AreaNumb==26|sum.by.year$AreaNumb==81|sum.by.year$AreaNumb==83|sum.by.year$AreaNumb==86|sum.by.year$AreaNumb==88|sum.by.year$AreaNumb==158|sum.by.year$AreaNumb==164|sum.by.year$AreaNumb==189|sum.by.year$AreaNumb==218|sum.by.year$AreaNumb==243|sum.by.year$AreaNumb==245|sum.by.year$AreaNumb==247|sum.by.year$AreaNumb==249|sum.by.year$AreaNumb==251|sum.by.year$AreaNumb==253|sum.by.year$AreaNumb==254|sum.by.year$AreaNumb==256|sum.by.year$AreaNumb==319|sum.by.year$AreaNumb==335|sum.by.year$AreaNumb==340|sum.by.year$AreaNumb==369|sum.by.year$AreaNumb==371|sum.by.year$AreaNumb==584|sum.by.year$AreaNumb==391|sum.by.year$AreaNumb==397|sum.by.year$AreaNumb==417|sum.by.year$AreaNumb==465|sum.by.year$AreaNumb==513|sum.by.year$AreaNumb==515|sum.by.year$AreaNumb==522|sum.by.year$AreaNumb==523|sum.by.year$AreaNumb==526|sum.by.year$AreaNumb==530|sum.by.year$AreaNumb==552|sum.by.year$AreaNumb==556|sum.by.year$AreaNumb==558|sum.by.year$AreaNumb==559|sum.by.year$AreaNumb==565|sum.by.year$AreaNumb==583|sum.by.year$AreaNumb==587|sum.by.year$AreaNumb==590|sum.by.year$AreaNumb==597|sum.by.year$AreaNumb==607|sum.by.year$AreaNumb==615|sum.by.year$AreaNumb==618|sum.by.year$AreaNumb==619|sum.by.year$AreaNumb==620|sum.by.year$AreaNumb==676|sum.by.year$AreaNumb==700|sum.by.year$AreaNumb==715|sum.by.year$AreaNumb==719|sum.by.year$AreaNumb==724|sum.by.year$AreaNumb==725|sum.by.year$AreaNumb==900|sum.by.year$AreaNumb==969|sum.by.year$AreaNumb==972|sum.by.year$AreaNumb==973|sum.by.year$AreaNumb==975|sum.by.year$AreaNumb==978|sum.by.year$AreaNumb==1931|sum.by.year$AreaNumb==1932|sum.by.year$AreaNumb==2041|sum.by.year$AreaNumb==3262|sum.by.year$AreaNumb==3333|sum.by.year$AreaNumb==3921|sum.by.year$AreaNumb==3922|sum.by.year$AreaNumb==4022|sum.by.year$AreaNumb==4130|sum.by.year$AreaNumb==8300|sum.by.year$AreaNumb==8301|sum.by.year$AreaNumb==8512|sum.by.year$AreaNumb==9007|sum.by.year$AreaNumb==9009|sum.by.year$AreaNumb==9014|sum.by.year$AreaNumb==9019|sum.by.year$AreaNumb==9098|sum.by.year$AreaNumb==9601|sum.by.year$AreaNumb==9970|sum.by.year$AreaNumb==9971),]
write.csv(final,"temp2.csv", row.names = FALSE)




























qiime feature-table filter-features \
  --i-table final-filtered-table-98.qza \
  --p-min-frequency 1018 \
  --o-filtered-table dom-table.qza

qiime diversity core-metrics-phylogenetic \
  --i-phylogeny rooted-tree.qza \
  --i-table dom-table.qza \
  --p-sampling-depth 1000 \
  --m-metadata-file metadata4_summary.txt \
  --output-dir core-metrics-results

qiime tools export \
  --input-path core-metrics-results/bray_curtis_distance_matrix.qza \
  --output-path core-metrics-results/bray_curtis_distance_matrix.tsv 

library(mvabund)
library(vegan)

data=read.csv("dom-bray-curtis-distance-matrix.csv",header=TRUE)
bray=dist(data[,2:57])
bray.single=hclust(bray,method='single')
bray.complete=hclust(bray,method='complete')
bray.UPGMA=hclust(bray,method='average')
bray.ward=hclust(bray,method='ward.D')
par(mfrow=c(2,2))
plot(bray.single);plot(bray.complete);plot(bray.UPGMA);plot(bray.ward)

# cophenetic distance
bray.single.coph=cophenetic(bray.single)
bray.complete.coph=cophenetic(bray.complete)
bray.UPGMA.coph=cophenetic(bray.UPGMA)
bray.ward.coph=cophenetic(bray.ward)

# plot
par(mfrow=c(2,2))

# single
plot(bray,bray.single.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Single',paste('Cophenetic correlation',round(cor(bray,bray.single.coph),3))))
abline(0,1)
lines(lowess(bray,bray.single.coph),col='red')

# complete
plot(bray,bray.complete.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Complete',paste('Cophenetic correlation',round(cor(bray,bray.complete.coph),3))))
abline(0,1)
lines(lowess(bray,bray.complete.coph),col='red')

# UPGMA
plot(bray,bray.UPGMA.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('UPGMA',paste('Cophenetic correlation',round(cor(bray,bray.UPGMA.coph),3))))
abline(0,1)
lines(lowess(bray,bray.UPGMA.coph),col='red')

# ward
plot(bray,bray.ward.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Ward',paste('Cophenetic correlation',round(cor(bray,bray.ward.coph),3))))
abline(0,1)
lines(lowess(bray,bray.ward.coph),col='red')

# Looking for interpretable clusters
library(factoextra)
fviz_nbclust(as.matrix(bray),hcut,method='silhouette',hc_method='average')
fviz_nbclust(as.matrix(bray),hcut,method='wss',hc_method='average')

grpdist=function(x){
	require(cluster)
	gr=as.data.frame(as.factor(x))
	distgr=daisy(gr,'gower')
	distgr
	}

kt=data.frame(k=1:nrow(data),r=0)
for (i in 2:(nrow(data)-1)){
	gr=cutree(bray.UPGMA,i)
	distgr=grpdist(gr)
	mt=cor(bray,distgr,method='pearson')
	kt[i,2]=mt
	}
k.best=which.max(kt$r)
plot(kt$k,kt$r,
	type='h',main='Mantel-optimal number of clusters-UPGMA',
	xlab='k (number of groups)',ylab="Pearson's correlation")
axis(1,k.best,
	paste('optimum',k.best,sep='\n'),col='red',font=2,col.axis='red')
points(k.best,max(kt$r),pch=16,col='red',cex=1.5)

cutg=cutree(bray.UPGMA,k=3)
sil=silhouette(cutg,bray)
plot(sil)


data=read.csv("bray-curtis-distance-matrix.csv",header=TRUE)
data=data[-c(34,35,60),] 
data=subset(data,select=-c(Dane975,Dane558,Dane559))
bray=dist(data[,2:60])
bray.single=hclust(bray,method='single')
bray.complete=hclust(bray,method='complete')
bray.UPGMA=hclust(bray,method='average')
bray.ward=hclust(bray,method='ward.D')
par(mfrow=c(2,2))
plot(bray.single);plot(bray.complete);plot(bray.UPGMA);plot(bray.ward)

# cophenetic distance
bray.single.coph=cophenetic(bray.single)
bray.complete.coph=cophenetic(bray.complete)
bray.UPGMA.coph=cophenetic(bray.UPGMA)
bray.ward.coph=cophenetic(bray.ward)

# plot
par(mfrow=c(2,2))

# single
plot(bray,bray.single.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Single',paste('Cophenetic correlation',round(cor(bray,bray.single.coph),3))))
abline(0,1)
lines(lowess(bray,bray.single.coph),col='red')

# complete
plot(bray,bray.complete.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Complete',paste('Cophenetic correlation',round(cor(bray,bray.complete.coph),3))))
abline(0,1)
lines(lowess(bray,bray.complete.coph),col='red')

# UPGMA
plot(bray,bray.UPGMA.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('UPGMA',paste('Cophenetic correlation',round(cor(bray,bray.UPGMA.coph),3))))
abline(0,1)
lines(lowess(bray,bray.UPGMA.coph),col='red')

# ward
plot(bray,bray.ward.coph,xlab='Bray distance',ylab='Cophenetic distance',asp=1,main=c('Ward',paste('Cophenetic correlation',round(cor(bray,bray.ward.coph),3))))
abline(0,1)
lines(lowess(bray,bray.ward.coph),col='red')

# Looking for interpretable clusters
library(factoextra)
fviz_nbclust(as.matrix(bray),hcut,method='silhouette',hc_method='average')
fviz_nbclust(as.matrix(bray),hcut,method='wss',hc_method='average')

grpdist=function(x){
	require(cluster)
	gr=as.data.frame(as.factor(x))
	distgr=daisy(gr,'gower')
	distgr
	}

kt=data.frame(k=1:nrow(data),r=0)
for (i in 2:(nrow(data)-1)){
	gr=cutree(bray.UPGMA,i)
	distgr=grpdist(gr)
	mt=cor(bray,distgr,method='pearson')
	kt[i,2]=mt
	}
k.best=which.max(kt$r)
plot(kt$k,kt$r,
	type='h',main='Mantel-optimal number of clusters-UPGMA',
	xlab='k (number of groups)',ylab="Pearson's correlation")
axis(1,k.best,
	paste('optimum',k.best,sep='\n'),col='red',font=2,col.axis='red')
points(k.best,max(kt$r),pch=16,col='red',cex=1.5)

cutg=cutree(bray.UPGMA,k=3)
sil=silhouette(cutg,bray)
plot(sil)












conda activate qiime2-2021.11

qiime diversity umap \
  --i-distance-matrix bray_curtis_distance_matrix.qza \
  --o-umap bray_curtis_umap_matrix.qza

qiime tools export \
  --input-path bray_curtis_umap_matrix.qza \
  --output-path bray_curtis_umap_matrix.tsv 

qiime diversity umap \
  --i-distance-matrix jaccard_distance_matrix.qza \
  --o-umap jaccard_umap_matrix.qza

qiime tools export \
  --input-path jaccard_umap_matrix.qza \
  --output-path jaccard_umap_matrix.tsv 
  
qiime diversity umap \
  --i-distance-matrix unweighted_unifrac_distance_matrix.qza \
  --o-umap unweighted_unifrac_umap_matrix.qza

qiime tools export \
  --input-path unweighted_unifrac_umap_matrix.qza \
  --output-path unweighted_unifrac_umap_matrix.tsv 

qiime diversity umap \
  --i-distance-matrix weighted_unifrac_distance_matrix.qza \
  --o-umap weighted_unifrac_umap_matrix.qza

qiime tools export \
  --input-path weighted_unifrac_umap_matrix.qza \
  --output-path weighted_unifrac_umap_matrix.tsv 

data=read.csv("Hist-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,Year,SampDate))
sum.by.site=aggregate(.~AreaNumb,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.site[is.na(sum.by.site)]=0  
sum.by.site$PercSiteDry=sum.by.site$SiteDry/sum.by.site$SiteVis
sum.by.site$PercLarvFnd=sum.by.site$LarvFnd/sum.by.site$SiteVis
sum.by.site$LarvDens=sum.by.site$TotCnt/sum.by.site$NumDips
sum.by.site$AeDens=sum.by.site$AeCnt/sum.by.site$NumDips
sum.by.site$AnDens=sum.by.site$AnCnt/sum.by.site$NumDips
sum.by.site$CxDens=sum.by.site$CxCnt/sum.by.site$NumDips
sum.by.site$PercTreat=sum.by.site$Treat/sum.by.site$SiteVis
sum.by.site$PercSiteDry[is.nan(sum.by.site$PercSiteDry)]=0
sum.by.site$PercLarvFnd[is.nan(sum.by.site$PercLarvFnd)]=0
sum.by.site$LarvDens[is.nan(sum.by.site$LarvDens)]=0
sum.by.site$AeDens[is.nan(sum.by.site$AeDens)]=0
sum.by.site$AnDens[is.nan(sum.by.site$AnDens)]=0
sum.by.site$CxDens[is.nan(sum.by.site$CxDens)]=0
sum.by.site$PercTreat[is.nan(sum.by.site$PercTreat)]=0
final=sum.by.site[which(sum.by.site$AreaNumb==4|sum.by.site$AreaNumb==5|sum.by.site$AreaNumb==6|sum.by.site$AreaNumb==13|sum.by.site$AreaNumb==26|sum.by.site$AreaNumb==81|sum.by.site$AreaNumb==86|sum.by.site$AreaNumb==88|sum.by.site$AreaNumb==164|sum.by.site$AreaNumb==189|sum.by.site$AreaNumb==243|sum.by.site$AreaNumb==245|sum.by.site$AreaNumb==249|sum.by.site$AreaNumb==251|sum.by.site$AreaNumb==253|sum.by.site$AreaNumb==254|sum.by.site$AreaNumb==256|sum.by.site$AreaNumb==335|sum.by.site$AreaNumb==340|sum.by.site$AreaNumb==371|sum.by.site$AreaNumb==397|sum.by.site$AreaNumb==417|sum.by.site$AreaNumb==465|sum.by.site$AreaNumb==513|sum.by.site$AreaNumb==515|sum.by.site$AreaNumb==522|sum.by.site$AreaNumb==523|sum.by.site$AreaNumb==552|sum.by.site$AreaNumb==558|sum.by.site$AreaNumb==559|sum.by.site$AreaNumb==565|sum.by.site$AreaNumb==584|sum.by.site$AreaNumb==590|sum.by.site$AreaNumb==597|sum.by.site$AreaNumb==607|sum.by.site$AreaNumb==618|sum.by.site$AreaNumb==619|sum.by.site$AreaNumb==620|sum.by.site$AreaNumb==700|sum.by.site$AreaNumb==715|sum.by.site$AreaNumb==718|sum.by.site$AreaNumb==719|sum.by.site$AreaNumb==725|sum.by.site$AreaNumb==900|sum.by.site$AreaNumb==969|sum.by.site$AreaNumb==972|sum.by.site$AreaNumb==973|sum.by.site$AreaNumb==975|sum.by.site$AreaNumb==978|sum.by.site$AreaNumb==1931|sum.by.site$AreaNumb==1932|sum.by.site$AreaNumb==2041|sum.by.site$AreaNumb==3262|sum.by.site$AreaNumb==3333|sum.by.site$AreaNumb==3922|sum.by.site$AreaNumb==4022|sum.by.site$AreaNumb==4130|sum.by.site$AreaNumb==8300|sum.by.site$AreaNumb==8512|sum.by.site$AreaNumb==9014|sum.by.site$AreaNumb==9098|sum.by.site$AreaNumb==9970),]
write.csv(final,"temp.csv", row.names = FALSE)

data=read.csv("Hist-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercSiteDry=sum.by.year$SiteDry/sum.by.year$SiteVis
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$AeDens=sum.by.year$AeCnt/sum.by.year$NumDips
sum.by.year$AnDens=sum.by.year$AnCnt/sum.by.year$NumDips
sum.by.year$CxDens=sum.by.year$CxCnt/sum.by.year$NumDips
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercSiteDry[is.nan(sum.by.year$PercSiteDry)]=0
sum.by.year$PercLarvFnd[is.nan(sum.by.year$PercLarvFnd)]=0
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
sum.by.year$AeDens[is.nan(sum.by.year$AeDens)]=0
sum.by.year$AnDens[is.nan(sum.by.year$AnDens)]=0
sum.by.year$CxDens[is.nan(sum.by.year$CxDens)]=0
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
final=sum.by.year[which(sum.by.year$AreaNumb==4|sum.by.year$AreaNumb==5|sum.by.year$AreaNumb==6|sum.by.year$AreaNumb==13|sum.by.year$AreaNumb==26|sum.by.year$AreaNumb==81|sum.by.year$AreaNumb==86|sum.by.year$AreaNumb==88|sum.by.year$AreaNumb==164|sum.by.year$AreaNumb==189|sum.by.year$AreaNumb==243|sum.by.year$AreaNumb==245|sum.by.year$AreaNumb==249|sum.by.year$AreaNumb==251|sum.by.year$AreaNumb==253|sum.by.year$AreaNumb==254|sum.by.year$AreaNumb==256|sum.by.year$AreaNumb==335|sum.by.year$AreaNumb==340|sum.by.year$AreaNumb==371|sum.by.year$AreaNumb==397|sum.by.year$AreaNumb==417|sum.by.year$AreaNumb==465|sum.by.year$AreaNumb==513|sum.by.year$AreaNumb==515|sum.by.year$AreaNumb==522|sum.by.year$AreaNumb==523|sum.by.year$AreaNumb==552|sum.by.year$AreaNumb==558|sum.by.year$AreaNumb==559|sum.by.year$AreaNumb==565|sum.by.year$AreaNumb==584|sum.by.year$AreaNumb==590|sum.by.year$AreaNumb==597|sum.by.year$AreaNumb==607|sum.by.year$AreaNumb==618|sum.by.year$AreaNumb==619|sum.by.year$AreaNumb==620|sum.by.year$AreaNumb==700|sum.by.year$AreaNumb==715|sum.by.year$AreaNumb==718|sum.by.year$AreaNumb==719|sum.by.year$AreaNumb==725|sum.by.year$AreaNumb==900|sum.by.year$AreaNumb==969|sum.by.year$AreaNumb==972|sum.by.year$AreaNumb==973|sum.by.year$AreaNumb==975|sum.by.year$AreaNumb==978|sum.by.year$AreaNumb==1931|sum.by.year$AreaNumb==1932|sum.by.year$AreaNumb==2041|sum.by.year$AreaNumb==3262|sum.by.year$AreaNumb==3333|sum.by.year$AreaNumb==3922|sum.by.year$AreaNumb==4022|sum.by.year$AreaNumb==4130|sum.by.year$AreaNumb==8300|sum.by.year$AreaNumb==8512|sum.by.year$AreaNumb==9014|sum.by.year$AreaNumb==9098|sum.by.year$AreaNumb==9970),]
write.csv(final,"temp2.csv", row.names = FALSE)

data=read.csv("Hist-MB-Comp-Collated-By-Site.csv",header=TRUE)
Site=as.factor(data$AreaNumb)
Effort=as.numeric(data$SiteVis)
Permanence=as.numeric(data$PercSiteDry)
LarvFrequency=as.numeric(data$PercLarvFnd)
LarvDens=as.numeric(data$LarvDens)
AeDens=as.numeric(data$AeDens)
AnDens=as.numeric(data$AnDens)
CxDens=as.numeric(data$CxDens)
PercTreat=as.numeric(data$PercTreat)
Evenness=as.numeric(data$pielou_evenness)
FaithPD=as.numeric(data$faith_pd)
Richness=as.numeric(data$observed_features)
Shannon=as.numeric(data$shannon_entropy)
BrayCurtisA=as.numeric(data$bray_curtis_1)
BrayCurtisB=as.numeric(data$bray_curtis_2)
JaccardA=as.numeric(data$jaccard_1)
JaccardB=as.numeric(data$jaccard_2)
UnweightedUnifracA=as.numeric(data$unweighted_unifrac_1)
UnweightedUnifracB=as.numeric(data$unweighted_unifrac_2)
WeightedUnifracA=as.numeric(data$weighted_unifrac_1)
WeightedUnifracB=as.numeric(data$weighted_unifrac_2)

library(car)
library(MASS)

model=glm(log10(Richness)~Permanence+LarvFrequency+LarvDens+PercTreat,family=gaussian("log"),offset=log(Effort))
vif(model)
summary(model)
model=glm(log10(Richness)~+LarvFrequency,family=gaussian("log"),offset=log(Effort))
summary(model) #LarvFrequency

model=glm(log10(observed_features)~SiteDry+LarvFnd+TotCnt+Treat,family=gaussian("log"),offset=log(SiteVis),data=data)
vif(model)
model=glm(log10(observed_features)~SiteDry+LarvFnd+TotCnt,family=gaussian("log"),offset=log(SiteVis),data=data)
vif(model)
summary(model)
model=glm(log10(observed_features)~SiteDry+LarvFnd+TotCnt,family=gaussian("log"),offset=log(SiteVis),data=data)
summary(model) #LarvFrequency

model=glm(Shannon~Permanence+LarvFrequency+LarvDens+PercTreat,family=gaussian("log"),offset=log(Effort))
vif(model)
summary(model)
model=glm(Shannon~Permanence+LarvFrequency,family=gaussian("log"),offset=log(Effort))
summary(model) #LarvFrequency

model=glm(shannon_entropy~SiteDry+LarvFnd+TotCnt+Treat,family=gaussian("log"),offset=log(SiteVis),data=data)
vif(model)
model=glm(shannon_entropy~SiteDry+LarvFnd+TotCnt,family=gaussian("log"),offset=log(SiteVis),data=data)
vif(model)
summary(model)
model=glm(shannon_entropy~LarvFnd,family=gaussian("log"),offset=log(SiteVis),data=data)
summary(model) #LarvFrequency

model=glm(BrayCurtisA~Permanence+LarvFrequency+LarvDens+PercTreat,family=gaussian("identity"),offset=log(Effort))
vif(model)
summary(model)
model=glm(BrayCurtisA~LarvFrequency+LarvDens,family=gaussian("identity"),offset=log(Effort))
summary(model) #LarvFrequency, LarvDens

model=glm(bray_curtis_1~SiteDry+LarvFnd+TotCnt+Treat,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(bray_curtis_1~SiteDry+LarvFnd+TotCnt,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(bray_curtis_1~SiteDry+LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(bray_curtis_1~SiteDry*LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
summary(model) #Permanence, LarvFrequency, Interaction

model=glm(BrayCurtisB~Permanence+LarvFrequency+LarvDens+PercTreat,family=gaussian("identity"),offset=log(Effort))
vif(model)
summary(model)  #Not significant

model=glm(bray_curtis_2~SiteDry+LarvFnd+TotCnt+Treat,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(bray_curtis_2~SiteDry+LarvFnd+TotCnt,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(bray_curtis_2~SiteDry+LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(bray_curtis_2~SiteDry*LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
summary(model)  #Permanence, LarvFrequency, Interaction

data=read.csv("Hist-MB-Comp-Collated-By-Site.csv",header=TRUE)
braycurtis.umap=read.csv("bray_curtis_umap_matrix.csv",header=TRUE)
braycurtis.umap.data=dplyr::inner_join(data,braycurtis.umap,by="AreaNumb")
  
library(ggplot2)
library(magrittr)

plot=ggplot(
  braycurtis.umap.data,
  aes(
    x = bray_curtis_1.y,
    y = bray_curtis_2.y,
    color = PercLarvFnd 
  )
) +
  geom_point(size = 4)
mid=median(unweightedunifrac.umap.data$PercLarvFnd)
plot+scale_color_gradient2(midpoint=mid,low="blue",mid="white",high="red")

model=glm(UnweightedUnifracA~Permanence+LarvFrequency+LarvDens+PercTreat,family=gaussian("identity"),offset=log(Effort))
vif(model)
summary(model) #Not significant

model=glm(unweighted_unifrac_1~SiteDry+LarvFnd+TotCnt+Treat,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(unweighted_unifrac_1~SiteDry+LarvFnd+TotCnt,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(unweighted_unifrac_1~SiteDry+LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(unweighted_unifrac_1~LarvFnd+SiteDry:LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
summary(model) #LarvFrequency

model=glm(UnweightedUnifracB~Permanence+LarvFrequency+LarvDens+PercTreat,family=gaussian("identity"),offset=log(Effort))
vif(model)
summary(model)
model=glm(UnweightedUnifracB~LarvFrequency+LarvDens,family=gaussian("identity"),offset=log(Effort))
summary(model) #LarvFrequency

model=glm(unweighted_unifrac_2~SiteDry+LarvFnd+TotCnt+Treat,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(unweighted_unifrac_2~SiteDry+LarvFnd+TotCnt,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
model=glm(unweighted_unifrac_2~SiteDry+LarvFnd,family=gaussian("identity"),offset=log(SiteVis),data=data)
vif(model)
summary(model) #Permanence, LarvFrequency

data=read.csv("Hist-MB-Comp-Collated-By-Site.csv",header=TRUE)
unweightedunifrac.umap=read.csv("unweighted_unifrac_umap_matrix.csv",header=TRUE)
unweightedunifrac.umap.data=dplyr::inner_join(data,unweightedunifrac.umap,by="AreaNumb")
  
library(ggplot2)
library(magrittr)

plot=ggplot(
  unweightedunifrac.umap.data,
  aes(
    x = unweighted_unifrac_1.y,
    y = unweighted_unifrac_2.y,
    color = PercLarvFnd 
  )
) +
  geom_point(size = 4)
mid=median(unweightedunifrac.umap.data$PercLarvFnd)
plot+scale_color_gradient2(midpoint=mid,low="blue",mid="white",high="red")

med=median(data$PercLarvFnd)
data$Prod=ifelse(data$PercLarvFnd<med,"Low","High")
data$Prod=as.factor(data$Prod)

braycurtis.umap.data=dplyr::inner_join(data,braycurtis.umap,by="AreaNumb")

plot=ggplot(
  braycurtis.umap.data,
  aes(
    x = bray_curtis_1.y,
    y = bray_curtis_2.y,
    color = Prod 
  )
) +
  geom_point(size = 4)
plot

library(ggplot2)
library(FSA)

unweightedunifrac.umap.data=dplyr::inner_join(data,unweightedunifrac.umap,by="AreaNumb")

plot=ggplot(
  unweightedunifrac.umap.data,
  aes(
    x = unweighted_unifrac_1.y,
    y = unweighted_unifrac_2.y,
    color = Prod 
  )
) +
  geom_point(size = 4)
plot

ggplot(data,aes(x=Prod,y=PercLarvFnd,fill=Prod))+geom_boxplot()+geom_jitter()
ggplot(data,aes(x=Prod,y=LarvDens,fill=Prod))+geom_boxplot()+geom_jitter()

ggplot(data,aes(x=Prod,y=log10(observed_features),fill=Prod))+geom_boxplot()+geom_jitter()
ggplot(data,aes(x=Prod,y=shannon_entropy,fill=Prod))+geom_boxplot()+geom_jitter()

Sum=Summarize(log10(observed_features)~Prod,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Prod,y=mean,color=Prod,data=Sum,ylim=c(2,2.3))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

Sum=Summarize(shannon_entropy~Prod,data=data)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Prod,y=mean,color=Prod,data=Sum,ylim=c(5,6.5))+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

data=read.csv("Hist-MB-Comp-Collated-By-Site-Year.csv",header=TRUE)
data$Wave=data$Year
data$Wave[data$Wave==2007]=1
data$Wave[data$Wave==2008]=2
data$Wave[data$Wave==2009]=3
data$Wave[data$Wave==2010]=4
data$Wave[data$Wave==2011]=5
data$Wave[data$Wave==2012]=6
data$Wave[data$Wave==2013]=7
data$Wave[data$Wave==2014]=8
data$Wave[data$Wave==2015]=9
data$Wave[data$Wave==2016]=10
data$Wave[data$Wave==2017]=11
data$Wave[data$Wave==2018]=12
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

fit.1<-geeglm(log10(observed_features)~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(log10(observed_features)~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(log10(observed_features)~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(log10(observed_features)~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="ar1",data=data)
summary(fit.3)

fit.1<-geeglm(shannon_entropy~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(shannon_entropy~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(shannon_entropy~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(shannon_entropy~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian(log),wave=Wave,corstr="ar1",data=data)
summary(fit.3)

fit.1<-geeglm(bray_curtis_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(bray_curtis_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(bray_curtis_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(bray_curtis_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
vif(fit.3)

fit.1<-geeglm(bray_curtis_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(bray_curtis_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(bray_curtis_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(bray_curtis_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)

fit.1<-geeglm(unweighted_unifrac_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(unweighted_unifrac_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(unweighted_unifrac_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(unweighted_unifrac_1~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)

fit.1<-geeglm(unweighted_unifrac_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="independence",data=data)
summary(fit.1)
fit.2<-geeglm(unweighted_unifrac_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="exchangeable",data=data)
summary(fit.2)
fit.3<-geeglm(unweighted_unifrac_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(unweighted_unifrac_2~PercLarvFnd+LarvDens+PercTreat,id=AreaNumb,family=gaussian,wave=Wave,corstr="ar1",data=data)
summary(fit.3)

data=read.csv("Curr-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercSiteDry=sum.by.year$SiteDry/sum.by.year$SiteVis
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$AeDens=sum.by.year$AeCnt/sum.by.year$NumDips
sum.by.year$AnDens=sum.by.year$AnCnt/sum.by.year$NumDips
sum.by.year$CxDens=sum.by.year$CxCnt/sum.by.year$NumDips
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercSiteDry[is.nan(sum.by.year$PercSiteDry)]=0
sum.by.year$PercLarvFnd[is.nan(sum.by.year$PercLarvFnd)]=0
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
sum.by.year$AeDens[is.nan(sum.by.year$AeDens)]=0
sum.by.year$AnDens[is.nan(sum.by.year$AnDens)]=0
sum.by.year$CxDens[is.nan(sum.by.year$CxDens)]=0
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
final=sum.by.year[which(sum.by.year$AreaNumb==4|sum.by.year$AreaNumb==5|sum.by.year$AreaNumb==6|sum.by.year$AreaNumb==13|sum.by.year$AreaNumb==26|sum.by.year$AreaNumb==81|sum.by.year$AreaNumb==86|sum.by.year$AreaNumb==88|sum.by.year$AreaNumb==164|sum.by.year$AreaNumb==189|sum.by.year$AreaNumb==243|sum.by.year$AreaNumb==245|sum.by.year$AreaNumb==249|sum.by.year$AreaNumb==251|sum.by.year$AreaNumb==253|sum.by.year$AreaNumb==254|sum.by.year$AreaNumb==256|sum.by.year$AreaNumb==335|sum.by.year$AreaNumb==340|sum.by.year$AreaNumb==371|sum.by.year$AreaNumb==397|sum.by.year$AreaNumb==417|sum.by.year$AreaNumb==465|sum.by.year$AreaNumb==513|sum.by.year$AreaNumb==515|sum.by.year$AreaNumb==522|sum.by.year$AreaNumb==523|sum.by.year$AreaNumb==552|sum.by.year$AreaNumb==558|sum.by.year$AreaNumb==559|sum.by.year$AreaNumb==565|sum.by.year$AreaNumb==584|sum.by.year$AreaNumb==590|sum.by.year$AreaNumb==597|sum.by.year$AreaNumb==607|sum.by.year$AreaNumb==618|sum.by.year$AreaNumb==619|sum.by.year$AreaNumb==620|sum.by.year$AreaNumb==700|sum.by.year$AreaNumb==715|sum.by.year$AreaNumb==718|sum.by.year$AreaNumb==719|sum.by.year$AreaNumb==725|sum.by.year$AreaNumb==900|sum.by.year$AreaNumb==969|sum.by.year$AreaNumb==972|sum.by.year$AreaNumb==973|sum.by.year$AreaNumb==975|sum.by.year$AreaNumb==978|sum.by.year$AreaNumb==1931|sum.by.year$AreaNumb==1932|sum.by.year$AreaNumb==2041|sum.by.year$AreaNumb==3262|sum.by.year$AreaNumb==3333|sum.by.year$AreaNumb==3922|sum.by.year$AreaNumb==4022|sum.by.year$AreaNumb==4130|sum.by.year$AreaNumb==8300|sum.by.year$AreaNumb==8512|sum.by.year$AreaNumb==9014|sum.by.year$AreaNumb==9098|sum.by.year$AreaNumb==9970),]
write.csv(final,"temp2.csv", row.names = FALSE)

data=read.csv("Curr-MB-Comp-Collated-By-Site-Year.csv",header=TRUE)
data$Wave=data$Year
data$Wave[data$Wave==2019]=1
data$Wave[data$Wave==2020]=2
data$Wave[data$Wave==2021]=3
data=data[order(data$AreaNumb,data$Wave),]

library(geepack)
library(car)
library(MESS)

fit.1<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(SiteVis))
summary(fit.1)
fit.2<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(SiteVis))
summary(fit.2)
fit.3<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(SiteVis))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(SiteVis))
summary(fit.2)
QIC(fit.2)

data.new=data
data.new$LarvFnd[data.new$LarvFnd==0]=0
data.new$LarvFnd[data.new$LarvFnd>0]=1
fit.1<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data.new,offset=log(SiteVis))
summary(fit.1)
fit.2<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data.new,offset=log(SiteVis))
summary(fit.2)
fit.3<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVis))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(LarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data.new,offset=log(SiteVis))
summary(fit.3)
QIC(fit.3)

fit.1<-geeglm(PercLarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercLarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercLarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~PercSiteDry+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
QIC(fit.2)

fit.1<-geeglm(PercTreat~PercSiteDry+LarvDens+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercTreat~PercSiteDry+LarvDens+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercTreat~PercSiteDry+LarvDens+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.3<-geeglm(PercTreat~PercSiteDry+LarvDens,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.3)

fit.1<-geeglm(LarvDens~PercSiteDry+PercTreat+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(SiteVis))
summary(fit.1)
fit.2<-geeglm(LarvDens~PercSiteDry+PercTreat+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,offset=log(SiteVis))
summary(fit.2)
fit.3<-geeglm(LarvDens~PercSiteDry+PercTreat+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,offset=log(SiteVis))
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.1<-geeglm(LarvDens~PercTreat+observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,offset=log(SiteVis))
summary(fit.1)
QIC(fit.1)
























fit.1<-geeglm(PercLarvFnd~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(PercLarvFnd~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(PercLarvFnd~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
vif(fit.2)

fit.1<-geeglm(LarvDens~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(LarvDens~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(LarvDens~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)
QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(PercLarvFnd~observed_features+shannon_entropy+bray_curtis_1+bray_curtis_2+unweighted_unifrac_1+unweighted_unifrac_2,id=AreaNumb,family=binomial,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
vif(fit.2)





fit.1<-glm(LarvFnd~observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2,data=data,weights=SiteVis)
summary(fit.1)
data$resid<-residuals(fit.1)
test<-reshape(data,idvar="AreaNumb",v.names="resid",timevar="Wave",direction="wide",drop=names(data)[2:42])
cor(test[,31:42],use="pairwise.complete")

fit.1<-geeglm(LarvFnd~observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="independence",data=data,weights=SiteVis)
summary(fit.1)
fit.2<-geeglm(LarvFnd~observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)
fit.3<-geeglm(LarvFnd~observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="ar1",data=data,weights=SiteVis)
summary(fit.3)

QIC(fit.1)
QIC(fit.2)
QIC(fit.3)
fit.2<-geeglm(LarvFnd~observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2,id=AreaNumb,family=poisson,wave=Wave,corstr="exchangeable",data=data,weights=SiteVis)
summary(fit.2)



mf=formula(LarvFnd~observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2)
geeInd=geeglm(mf,id=AreaNumb,data=data,family=poisson,corstr="ar1",waves=Wave,weights=SiteVis)
vif(geeInd)
summary(geeInd)
QIC(geeInd)




mf=formula(Prod~Year+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2)
geeInd=geeglm(mf,id=AreaNumb,data=data,family=binomial,corstr="ar1",waves=Wave,offset=log(SiteVis),scale.fix=TRUE)
vif(geeInd)
summary(geeInd)
QIC(geeInd)
mf=formula(Prod~observed_features+shannon_entropy+unweighted_unifrac_2)
geeInd=geeglm(mf,id=AreaNumb,data=data,family=binomial,corstr="ar1",waves=Wave,offset=log(SiteVis),scale.fix=TRUE)
summary(geeInd) #Richness, Shannon, Unweighted Unifrac
QIC(geeInd)




data=read.csv("Hist-MB-Comp-Collated-By-Site-Year.csv",header=TRUE)
data$off=offset(log(data$SiteVis))
data$Wave=data$Year
data$Wave[data$Wave==2007]=1
data$Wave[data$Wave==2008]=2
data$Wave[data$Wave==2009]=3
data$Wave[data$Wave==2010]=4
data$Wave[data$Wave==2011]=5
data$Wave[data$Wave==2012]=6
data$Wave[data$Wave==2013]=7
data$Wave[data$Wave==2014]=8
data$Wave[data$Wave==2015]=9
data$Wave[data$Wave==2016]=10
data$Wave[data$Wave==2017]=11
data$Wave[data$Wave==2018]=12
data=data[order(data$AreaNumb,data$Wave),]
data$Prod=ifelse(data$PercLarvFnd<med.one&data$LarvDens<med.two,0,1)
data$Prod=as.numeric(data$Prod)

library(geepack)

mf=formula(Prod~Year+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_2)
geeInd=geeglm(mf,id=AreaNumb,data=data,family=binomial,corstr="ar1",waves=Wave,offset=log(SiteVis),scale.fix=TRUE)
vif(geeInd)
summary(geeInd)
QIC(geeInd)
mf=formula(Prod~observed_features+shannon_entropy+unweighted_unifrac_2)
geeInd=geeglm(mf,id=AreaNumb,data=data,family=binomial,corstr="ar1",waves=Wave,offset=log(SiteVis),scale.fix=TRUE)
summary(geeInd) #Richness, Shannon, Unweighted Unifrac
QIC(geeInd)







library(geeM)
library(MASS)
library(car)

model=geem(LarvFnd~SiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_1+off,id=AreaNumb,data=data,family=MASS::negative.binomial(2),corstr="ar1",waves=Wave)
summary(model) #Permanence, Richness




library(multgee)

mf=formula(Prod~observed_features+shannon_entropy)
geeInd=nomLORgee(mf,id=AreaNumb,data=data,repeated=Wave,LORstr="independence")
summary(geeInd)



newdata = with(data,data.frame(y=Cluster, id=Site, repeated=Year))
intrinsic.pars(y=y, data=newdata, id=id, repeated=repeated,rscale='nominal')




library(geepack)
library(geeM)
library(MASS)
library(car)

model=geem(LarvFnd~SiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_1+off,id=AreaNumb,data=data,family=MASS::negative.binomial(2),corstr="ar1",waves=Wave)
summary(model) #Permanence, Richness

model=geem(AeCnt~SiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_1+off,id=AreaNumb,data=data,family=MASS::negative.binomial(2),corstr="ar1",waves=Wave)
summary(model) #Shannon

model=geem(AnCnt~SiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_1+off,id=AreaNumb,data=data,family=MASS::negative.binomial(2),corstr="ar1",waves=Wave)
summary(model) #Unweighted Unifrac

model=geem(CxCnt~SiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_1+off,id=AreaNumb,data=data,family=MASS::negative.binomial(2),corstr="ar1",waves=Wave)
summary(model) #Permanence, Richness, Shannon

model=geem(Treat~SiteDry+observed_features+shannon_entropy+bray_curtis_1+unweighted_unifrac_1+off,id=AreaNumb,data=data,family=MASS::negative.binomial(2),corstr="ar1",waves=Wave)
summary(model) #Permanence

















mf=formula(LarvFrequency~Permanence+Richness+Shannon+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(LarvFrequency~Permanence+Richness+Shannon+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(LarvFrequency~Permanence+Richness+Shannon+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)




mf=formula(LarvFrequency~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
summary(geeInd)
QIC(geeInd)
mf=formula(LarvFrequency~Permanence+Richness+Shannon+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(LarvFrequency~Permanence+Richness+Shannon+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(LarvFrequency~Permanence+Richness+Shannon+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)

mf=formula(AeCnt~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
mf=formula(AeCnt~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
summary(geeInd)
QIC(geeInd)

mf=formula(AeCnt~Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(AeCnt~Richness+Shannon+BrayCurtisB+UnweightedUnifracA)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(AeCnt~Richness+Shannon+BrayCurtisB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(AeCnt~Richness+Shannon)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)
mf=formula(AeCnt~Richness)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
summary(geeInd)
QIC(geeInd)




mf=formula(AnCnt~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
mf=formula(AnCnt~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
summary(geeInd)

mf=formula(CxCnt~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
summary(geeInd)

mf=formula(TrtFrequency~Permanence+Richness+Shannon+BrayCurtisA+BrayCurtisB+UnweightedUnifracA+UnweightedUnifracB)
geeInd=geeglm(mf,id=Site,data=data,family=poisson,corstr="ar1",waves=Wave,offset=log(SiteVis))
vif(geeInd)
summary(geeInd)








































data=read.csv("MB-Beta-Working.csv",header=TRUE)
data.nc=data[,2:3]

library(factoextra)
library(cluster)

set.seed(123)
fviz_nbclust(data.nc,kmeans,method="wss")
set.seed(123)
fviz_nbclust(data.nc,kmeans,method="silhouette")
set.seed(123)
gap_stat=clusGap(data.nc,FUN=kmeans,nstart=25,K.max=10,B=50)
fviz_gap_stat(gap_stat)

data.clust=kmeans(data.nc,centers=2,nstart=25)
fviz_cluster(data.clust,data=data.nc)
data$BrayCurtis_Clust=data.clust$cluster

data.nc=data[,6:7]
set.seed(123)
fviz_nbclust(data.nc,kmeans,method="wss")
set.seed(123)
fviz_nbclust(data.nc,kmeans,method="silhouette")
set.seed(123)
gap_stat=clusGap(data.nc,FUN=kmeans,nstart=25,K.max=10,B=50)
fviz_gap_stat(gap_stat)

data.clust=kmeans(data.nc,centers=3,nstart=25)
fviz_cluster(data.clust,data=data.nc)
data$UnweightedUnifrac_Clust=data.clust$cluster

write.csv(data,"temp.csv", row.names = FALSE)

data=read.csv("Hist-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercSiteDry=sum.by.year$SiteDry/sum.by.year$SiteVis
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$AeDens=sum.by.year$AeCnt/sum.by.year$NumDips
sum.by.year$AnDens=sum.by.year$AnCnt/sum.by.year$NumDips
sum.by.year$CxDens=sum.by.year$CxCnt/sum.by.year$NumDips
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercSiteDry[is.nan(sum.by.year$PercSiteDry)]=0
sum.by.year$PercLarvFnd[is.nan(sum.by.year$PercLarvFnd)]=0
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
sum.by.year$AeDens[is.nan(sum.by.year$AeDens)]=0
sum.by.year$AnDens[is.nan(sum.by.year$AnDens)]=0
sum.by.year$CxDens[is.nan(sum.by.year$CxDens)]=0
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
final=sum.by.year[which(sum.by.year$AreaNumb==4|sum.by.year$AreaNumb==5|sum.by.year$AreaNumb==6|sum.by.year$AreaNumb==13|sum.by.year$AreaNumb==26|sum.by.year$AreaNumb==81|sum.by.year$AreaNumb==86|sum.by.year$AreaNumb==88|sum.by.year$AreaNumb==164|sum.by.year$AreaNumb==189|sum.by.year$AreaNumb==243|sum.by.year$AreaNumb==245|sum.by.year$AreaNumb==249|sum.by.year$AreaNumb==251|sum.by.year$AreaNumb==253|sum.by.year$AreaNumb==254|sum.by.year$AreaNumb==256|sum.by.year$AreaNumb==335|sum.by.year$AreaNumb==340|sum.by.year$AreaNumb==371|sum.by.year$AreaNumb==397|sum.by.year$AreaNumb==417|sum.by.year$AreaNumb==465|sum.by.year$AreaNumb==513|sum.by.year$AreaNumb==515|sum.by.year$AreaNumb==522|sum.by.year$AreaNumb==523|sum.by.year$AreaNumb==552|sum.by.year$AreaNumb==558|sum.by.year$AreaNumb==559|sum.by.year$AreaNumb==565|sum.by.year$AreaNumb==584|sum.by.year$AreaNumb==590|sum.by.year$AreaNumb==597|sum.by.year$AreaNumb==607|sum.by.year$AreaNumb==618|sum.by.year$AreaNumb==619|sum.by.year$AreaNumb==620|sum.by.year$AreaNumb==700|sum.by.year$AreaNumb==715|sum.by.year$AreaNumb==718|sum.by.year$AreaNumb==719|sum.by.year$AreaNumb==725|sum.by.year$AreaNumb==900|sum.by.year$AreaNumb==969|sum.by.year$AreaNumb==972|sum.by.year$AreaNumb==973|sum.by.year$AreaNumb==975|sum.by.year$AreaNumb==978|sum.by.year$AreaNumb==1931|sum.by.year$AreaNumb==1932|sum.by.year$AreaNumb==2041|sum.by.year$AreaNumb==3262|sum.by.year$AreaNumb==3333|sum.by.year$AreaNumb==3922|sum.by.year$AreaNumb==4022|sum.by.year$AreaNumb==4130|sum.by.year$AreaNumb==8300|sum.by.year$AreaNumb==8512|sum.by.year$AreaNumb==9014|sum.by.year$AreaNumb==9098|sum.by.year$AreaNumb==9970),]
write.csv(final,"temp2.csv", row.names = FALSE)






final=na.omit(sum.by.year)


library(geepack)

site=as.numeric(data$Site)
year=as.numeric(data$Year)
permanence=as.numeric(data$PercDry)
phmdc=as.numeric(data$phmdc_high)
avglarvdensity=as.numeric(data$AvgLarvaePerDip)
maxlarvdensity=as.numeric(data$MaxLarvaePerDip)
avglarvdiversity=as.numeric(data$AvgSpeciesPerDip)
maxlarvdiversity=as.numeric(data$MaxSpeciesPerDip)
larvfrequency=as.numeric(data$LarvaePA)
sitetype=as.factor(data$site_type)
evenness=as.numeric(data$evenness)
faithpd=as.numeric(data$faith_pd)
richness=as.numeric(data$observed_features)
shannon=as.numeric(data$shannon)
braycurtis=cbind(data$bray_curtis_1,data$bray_curtis_2)
bactdensity=as.numeric(data$log_bacterial_density)


mf.one=formula(phmdc~year+sitetype+evenness+faithpd+richness+shannon+braycurtis+bactdensity)
geeInd.one=geeglm(mf.one,id=site,data=data,family=binomial,corstr="ind")
summary(geeInd.one)

library(spind)
library(car)











set.seed(1)
data.clust=NbClust(data = data.nc, diss = NULL, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
final$Cluster=as.integer(data.clust$Best.partition)
write.csv(final,"temp.csv", row.names = FALSE)



data=read.csv("Hist-Master-Final.csv",header=TRUE)
data.new=subset(data,select=-c(DipID,SampDate))
sum.by.year=aggregate(.~AreaNumb+Year,data.new,function(x) sum(x,na.rm=TRUE),na.action=na.pass)
sum.by.year[is.na(sum.by.year)]=0  
sum.by.year$PercSiteDry=sum.by.year$SiteDry/sum.by.year$SiteVis
sum.by.year$PercLarvFnd=sum.by.year$LarvFnd/sum.by.year$SiteVis
sum.by.year$LarvDens=sum.by.year$TotCnt/sum.by.year$NumDips
sum.by.year$AeDens=sum.by.year$AeCnt/sum.by.year$NumDips
sum.by.year$AnDens=sum.by.year$AnCnt/sum.by.year$NumDips
sum.by.year$CxDens=sum.by.year$CxCnt/sum.by.year$NumDips
sum.by.year$NonVecDens=(sum.by.year$TotCnt-sum.by.year$CxCnt)/sum.by.year$NumDips
sum.by.year$PercTreat=sum.by.year$Treat/sum.by.year$SiteVis
sum.by.year$PercSiteDry[is.nan(sum.by.year$PercSiteDry)]=0
sum.by.year$PercLarvFnd[is.nan(sum.by.year$PercLarvFnd)]=0
sum.by.year$LarvDens[is.nan(sum.by.year$LarvDens)]=0
sum.by.year$AeDens[is.nan(sum.by.year$AeDens)]=0
sum.by.year$AnDens[is.nan(sum.by.year$AnDens)]=0
sum.by.year$CxDens[is.nan(sum.by.year$CxDens)]=0
sum.by.year$PercTreat[is.nan(sum.by.year$PercTreat)]=0
final=na.omit(sum.by.year)

library(umap)
custom.config = umap.defaults

data.only=subset(final,select=c(PercLarvFnd,LarvDens))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.dist=as.matrix(data.dist)
data.umap=umap(data.dist,config=custom.config,input="dist")

library(NbClust)

data.nc=data.umap$layout
set.seed(1)
data.clust=NbClust(data = data.nc, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
final$Cluster=as.integer(data.clust$Best.partition)
write.csv(final,"temp.csv", row.names = FALSE)

library(coin)
library(FSA)

cor.test(final$PercSiteDry,final$PercLarvFnd,method="kendall")
cor.test(final$PercSiteDry,final$LarvDens,method="kendall")
cor.test(final$PercSiteDry,final$PercTreat,method="kendall") #Not significant
cor.test(final$PercLarvFnd,final$LarvDens,method="kendall") #Not significant
cor.test(final$PercLarvFnd,final$PercTreat,method="kendall")
cor.test(final$LarvDens,final$PercTreat,method="kendall")

kruskal_test(PercSiteDry~as.factor(Cluster),data=final,distribution=approximate(nresample=1000)) #Not significant
kruskal_test(PercLarvFnd~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(PercLarvFnd~as.factor(Cluster),data=final,method="bh")
kruskal_test(LarvDens~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=final,method="bh")
kruskal_test(PercTreat~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(PercTreat~as.factor(Cluster),data=final,method="bh")



library(NbClust)

set.seed(1)
data.only=subset(final,select=c(PercSiteDry,PercLarvFnd,LarvDens,PercTreat))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
final$Cluster=as.integer(data.nc$Best.partition)
write.csv(final,"temp.csv", row.names = FALSE)

library(xlsx)
write.xlsx(dataframe1, file="filename.xlsx", sheetName="sheet1", row.names=FALSE)
write.xlsx(dataframe2, file="filename.xlsx", sheetName="sheet2", append=TRUE, row.names=FALSE)


cor.test(final$PercSiteDry,final$PercLarvFnd,method="kendall")
cor.test(final$PercSiteDry,final$LarvDens,method="kendall")
cor.test(final$PercSiteDry,final$PercTreat,method="kendall") #Not significant
cor.test(final$PercLarvFnd,final$LarvDens,method="kendall") #Not significant
cor.test(final$PercLarvFnd,final$PercTreat,method="kendall")
cor.test(final$LarvDens,final$PercTreat,method="kendall")

Cluster with PercLarvFnd and LarvDens

library(NbClust)

set.seed(1)
data.only=subset(final,select=c(PercLarvFnd,LarvDens))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
final$Cluster=as.integer(data.nc$Best.partition)
write.csv(final,"temp.csv", row.names = FALSE)

library(coin)
library(FSA)

kruskal_test(PercSiteDry~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(PercSiteDry~as.factor(Cluster),data=final,method="bh")

kruskal_test(PercLarvFnd~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(PercLarvFnd~as.factor(Cluster),data=final,method="bh")

kruskal_test(LarvDens~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=final,method="bh")

kruskal_test(PercTreat~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(PercTreat~as.factor(Cluster),data=final,method="bh")

newdata=final[which(final$AreaNumb==4|final$AreaNumb==5|final$AreaNumb==6|final$AreaNumb==13|final$AreaNumb==26|final$AreaNumb==81|final$AreaNumb==86|final$AreaNumb==88|final$AreaNumb==164|final$AreaNumb==189|final$AreaNumb==243|final$AreaNumb==245|final$AreaNumb==249|final$AreaNumb==251|final$AreaNumb==253|final$AreaNumb==254|final$AreaNumb==256|final$AreaNumb==335|final$AreaNumb==340|final$AreaNumb==371|final$AreaNumb==397|final$AreaNumb==417|final$AreaNumb==465|final$AreaNumb==513|final$AreaNumb==515|final$AreaNumb==522|final$AreaNumb==523|final$AreaNumb==552|final$AreaNumb==558|final$AreaNumb==559|final$AreaNumb==565|final$AreaNumb==584|final$AreaNumb==590|final$AreaNumb==597|final$AreaNumb==607|final$AreaNumb==618|final$AreaNumb==619|final$AreaNumb==620|final$AreaNumb==700|final$AreaNumb==715|final$AreaNumb==718|final$AreaNumb==719|final$AreaNumb==725|final$AreaNumb==900|final$AreaNumb==969|final$AreaNumb==972|final$AreaNumb==973|final$AreaNumb==975|final$AreaNumb==978|final$AreaNumb==1931|final$AreaNumb==1932|final$AreaNumb==2041|final$AreaNumb==3262|final$AreaNumb==3333|final$AreaNumb==3922|final$AreaNumb==4022|final$AreaNumb==4130|final$AreaNumb==8300|final$AreaNumb==8512|final$AreaNumb==9014|final$AreaNumb==9098|final$AreaNumb==9970),]
write.csv(newdata,"temp2.csv", row.names = FALSE)

library(NbClust)

newdata=final[which(final$AreaNumb==4|final$AreaNumb==5|final$AreaNumb==6|final$AreaNumb==13|final$AreaNumb==26|final$AreaNumb==81|final$AreaNumb==86|final$AreaNumb==88|final$AreaNumb==164|final$AreaNumb==189|final$AreaNumb==243|final$AreaNumb==245|final$AreaNumb==249|final$AreaNumb==251|final$AreaNumb==253|final$AreaNumb==254|final$AreaNumb==256|final$AreaNumb==335|final$AreaNumb==340|final$AreaNumb==371|final$AreaNumb==397|final$AreaNumb==417|final$AreaNumb==465|final$AreaNumb==513|final$AreaNumb==515|final$AreaNumb==522|final$AreaNumb==523|final$AreaNumb==552|final$AreaNumb==558|final$AreaNumb==559|final$AreaNumb==565|final$AreaNumb==584|final$AreaNumb==590|final$AreaNumb==597|final$AreaNumb==607|final$AreaNumb==618|final$AreaNumb==619|final$AreaNumb==620|final$AreaNumb==700|final$AreaNumb==715|final$AreaNumb==718|final$AreaNumb==719|final$AreaNumb==725|final$AreaNumb==900|final$AreaNumb==969|final$AreaNumb==972|final$AreaNumb==973|final$AreaNumb==975|final$AreaNumb==978|final$AreaNumb==1931|final$AreaNumb==1932|final$AreaNumb==2041|final$AreaNumb==3262|final$AreaNumb==3333|final$AreaNumb==3922|final$AreaNumb==4022|final$AreaNumb==4130|final$AreaNumb==8300|final$AreaNumb==8512|final$AreaNumb==9014|final$AreaNumb==9098|final$AreaNumb==9970),]

cor.test(newdata$PercSiteDry,newdata$PercLarvFnd,method="kendall")
cor.test(newdata$PercSiteDry,newdata$LarvDens,method="kendall") #Not significant
cor.test(newdata$PercSiteDry,newdata$PercTreat,method="kendall")
cor.test(newdata$PercLarvFnd,newdata$LarvDens,method="kendall") #Not significant
cor.test(newdata$PercLarvFnd,newdata$PercTreat,method="kendall")
cor.test(newdata$LarvDens,newdata$PercTreat,method="kendall")

#Cluster by PercLarvFnd, LarvDens

set.seed(1)
data.only=subset(newdata,select=c(PercLarvFnd,LarvDens))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
newdata$Cluster=as.integer(data.nc$Best.partition)
write.csv(newdata,"temp.csv", row.names = FALSE)
newdata$Cluster[newdata$Cluster==1]='High'
newdata$Cluster[newdata$Cluster==2]='Low'
write.csv(newdata,"temp.csv", row.names = FALSE)






library(NbClust)

set.seed(1)
newdata=final[which(final$AreaNumb==4|final$AreaNumb==5|final$AreaNumb==6|final$AreaNumb==13|final$AreaNumb==26|final$AreaNumb==81|final$AreaNumb==86|final$AreaNumb==88|final$AreaNumb==164|final$AreaNumb==189|final$AreaNumb==243|final$AreaNumb==245|final$AreaNumb==249|final$AreaNumb==251|final$AreaNumb==253|final$AreaNumb==254|final$AreaNumb==256|final$AreaNumb==335|final$AreaNumb==340|final$AreaNumb==371|final$AreaNumb==397|final$AreaNumb==417|final$AreaNumb==465|final$AreaNumb==513|final$AreaNumb==515|final$AreaNumb==522|final$AreaNumb==523|final$AreaNumb==552|final$AreaNumb==558|final$AreaNumb==559|final$AreaNumb==565|final$AreaNumb==584|final$AreaNumb==590|final$AreaNumb==597|final$AreaNumb==607|final$AreaNumb==618|final$AreaNumb==619|final$AreaNumb==620|final$AreaNumb==700|final$AreaNumb==715|final$AreaNumb==718|final$AreaNumb==719|final$AreaNumb==725|final$AreaNumb==900|final$AreaNumb==969|final$AreaNumb==972|final$AreaNumb==973|final$AreaNumb==975|final$AreaNumb==978|final$AreaNumb==1931|final$AreaNumb==1932|final$AreaNumb==2041|final$AreaNumb==3262|final$AreaNumb==3333|final$AreaNumb==3922|final$AreaNumb==4022|final$AreaNumb==4130|final$AreaNumb==8300|final$AreaNumb==8512|final$AreaNumb==9014|final$AreaNumb==9098|final$AreaNumb==9970),]
data.only=subset(newdata,select=c(PercLarvFnd,PercTreat))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
newdata$Cluster=as.integer(data.nc$Best.partition)
write.csv(newdata,"temp.csv", row.names = FALSE)
newdata$Cluster[newdata$Cluster==1]='High'
newdata$Cluster[newdata$Cluster==2]='Low'
write.csv(newdata,"temp.csv", row.names = FALSE)

library(coin)
library(FSA)

kruskal_test(PercLarvFnd~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))



library(NbClust)

set.seed(1)
data.only=subset(final,select=c(PercLarvFnd))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
final$Cluster=as.integer(data.nc$Best.partition)
write.csv(final,"temp.csv", row.names = FALSE)
final$Cluster[final$Cluster==1]='Low'
final$Cluster[final$Cluster==2]='High'
final$Cluster[final$Cluster==3]='Med'
write.csv(final,"temp.csv", row.names = FALSE)

library(coin)
library(FSA)

kruskal_test(PercLarvFnd~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
dunnTest(PercLarvFnd~as.factor(Cluster),data=final,method="bh")






newdata=final[which(final$AreaNumb==4|final$AreaNumb==5|final$AreaNumb==6|final$AreaNumb==13|final$AreaNumb==26|final$AreaNumb==81|final$AreaNumb==86|final$AreaNumb==88|final$AreaNumb==164|final$AreaNumb==189|final$AreaNumb==243|final$AreaNumb==245|final$AreaNumb==249|final$AreaNumb==251|final$AreaNumb==253|final$AreaNumb==254|final$AreaNumb==256|final$AreaNumb==335|final$AreaNumb==340|final$AreaNumb==371|final$AreaNumb==397|final$AreaNumb==417|final$AreaNumb==465|final$AreaNumb==513|final$AreaNumb==515|final$AreaNumb==522|final$AreaNumb==523|final$AreaNumb==552|final$AreaNumb==558|final$AreaNumb==559|final$AreaNumb==565|final$AreaNumb==584|final$AreaNumb==590|final$AreaNumb==597|final$AreaNumb==607|final$AreaNumb==618|final$AreaNumb==619|final$AreaNumb==620|final$AreaNumb==700|final$AreaNumb==715|final$AreaNumb==718|final$AreaNumb==719|final$AreaNumb==725|final$AreaNumb==900|final$AreaNumb==969|final$AreaNumb==972|final$AreaNumb==973|final$AreaNumb==975|final$AreaNumb==978|final$AreaNumb==1931|final$AreaNumb==1932|final$AreaNumb==2041|final$AreaNumb==3262|final$AreaNumb==3333|final$AreaNumb==3922|final$AreaNumb==4022|final$AreaNumb==4130|final$AreaNumb==8300|final$AreaNumb==8512|final$AreaNumb==9014|final$AreaNumb==9098|final$AreaNumb==9970),]



newdata=final[which(final$AreaNumb==4|final$AreaNumb==5|final$AreaNumb==6|final$AreaNumb==13|final$AreaNumb==26|final$AreaNumb==81|final$AreaNumb==86|final$AreaNumb==88|final$AreaNumb==164|final$AreaNumb==189|final$AreaNumb==243|final$AreaNumb==245|final$AreaNumb==249|final$AreaNumb==251|final$AreaNumb==253|final$AreaNumb==254|final$AreaNumb==256|final$AreaNumb==335|final$AreaNumb==340|final$AreaNumb==371|final$AreaNumb==397|final$AreaNumb==417|final$AreaNumb==465|final$AreaNumb==513|final$AreaNumb==515|final$AreaNumb==522|final$AreaNumb==523|final$AreaNumb==552|final$AreaNumb==558|final$AreaNumb==559|final$AreaNumb==565|final$AreaNumb==584|final$AreaNumb==590|final$AreaNumb==597|final$AreaNumb==607|final$AreaNumb==618|final$AreaNumb==619|final$AreaNumb==620|final$AreaNumb==700|final$AreaNumb==715|final$AreaNumb==718|final$AreaNumb==719|final$AreaNumb==725|final$AreaNumb==900|final$AreaNumb==969|final$AreaNumb==972|final$AreaNumb==973|final$AreaNumb==975|final$AreaNumb==978|final$AreaNumb==1931|final$AreaNumb==1932|final$AreaNumb==2041|final$AreaNumb==3262|final$AreaNumb==3333|final$AreaNumb==3922|final$AreaNumb==4022|final$AreaNumb==4130|final$AreaNumb==8300|final$AreaNumb==8512|final$AreaNumb==9014|final$AreaNumb==9098|final$AreaNumb==9970),]
newdata=newdata[which(newdata$PercSiteDry!=1),]
set.seed(1)
data.only=subset(newdata,select=c(PercLarvFnd,LarvDens))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
newdata$Cluster=as.integer(data.nc$Best.partition)
write.csv(newdata,"temp.csv", row.names = FALSE)



newdata$Cluster=as.integer(data.nc$Best.partition)
newdata$Cluster[newdata$Cluster==1]='High'
newdata$Cluster[newdata$Cluster==2]='Med'
newdata$Cluster[newdata$Cluster==3]='Low'
write.csv(newdata,"temp.csv", row.names = FALSE)

library(coin)
library(FSA)

kruskal_test(PercSiteDry~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))
dunnTest(PercSiteDry~as.factor(Cluster),data=newdata,method="bh")

kruskal_test(LarvDens~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=newdata,method="bh")

kruskal_test(AeDens~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=newdata,method="bh")

kruskal_test(AnDens~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=newdata,method="bh")

kruskal_test(CxDens~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=newdata,method="bh")

kruskal_test(PercTreat~as.factor(Cluster),data=newdata,distribution=approximate(nresample=1000))
dunnTest(LarvDens~as.factor(Cluster),data=newdata,method="bh")


temp=newdata
temp$Cluster[temp$Cluster=='Med']='High'

kruskal_test(PercSiteDry~as.factor(Cluster),data=temp,distribution=approximate(nresample=1000))

kruskal_test(LarvDens~as.factor(Cluster),data=temp,distribution=approximate(nresample=1000))

kruskal_test(PercTreat~as.factor(Cluster),data=temp,distribution=approximate(nresample=1000))







library(ggplot2)
library(ggfortify)

data.pca=prcomp(data.dist,center=TRUE,scale=TRUE)
summary(data.pca)
autoplot(data.pca,data=final,colour='Cluster')

final$Cluster[final$Cluster==1]='Low'
final$Cluster[final$Cluster==2]='High'
final$Cluster[final$Cluster==3]='Med'

library(coin)

kruskal_test(PercSiteDry~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=final,distribution=approximate(nresample=1000))

library(FSA)

dunnTest(AvgLarvDens~as.factor(Cluster),data=final,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=final,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=final,method="bh")



















library(NbClust)

set.seed(1)
data.only=subset(final,select=c(PercLarvFnd))
data.dist=dist(data.only,method="euclidean",diag=FALSE)
data.nc=NbClust(data = data.only, diss = data.dist, distance = NULL, min.nc = 2, max.nc = 15, method = 'kmeans', index = "all")
final$Cluster=as.integer(data.nc$Best.partition)
final$Cluster[final$Cluster==1]='Low'
final$Cluster[final$Cluster==2]='High'
final$Cluster[final$Cluster==3]='Med'

#write.csv(final,"temp.csv", row.names = FALSE)

sub.one=final[which(final$Year==2007),]
sub.two=final[which(final$Year==2008),]
sub.three=final[which(final$Year==2009),]
sub.four=final[which(final$Year==2010),]
sub.five=final[which(final$Year==2011),]
sub.six=final[which(final$Year==2012),]
sub.seven=final[which(final$Year==2013),]
sub.eight=final[which(final$Year==2014),]
sub.nine=final[which(final$Year==2015),]
sub.ten=final[which(final$Year==2016),]
sub.eleven=final[which(final$Year==2017),]
sub.twelve=final[which(final$Year==2018),]

library(coin)

kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.one,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.two,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.three,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.four,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.five,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.six,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.seven,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.eight,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.nine,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.ten,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.eleven,distribution=approximate(nresample=1000))
kruskal_test(PercLarvFnd~as.factor(Cluster),data=sub.twelve,distribution=approximate(nresample=1000))

library(FSA)

dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.one,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.two,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.three,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.four,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.five,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.six,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.seven,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.eight,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.nine,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.ten,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.eleven,method="bh")
dunnTest(PercLarvFnd~as.factor(Cluster),data=sub.twelve,method="bh")

Sum=Summarize(PercLarvFnd~Year+Cluster,data=final)
Sum$se=Sum$sd/sqrt(Sum$n)

library(ggplot2)

qplot(x=Year,y=mean,color=Cluster,data=Sum)+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.one,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.two,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.three,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.four,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.five,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.six,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.seven,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.eight,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.nine,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.ten,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.eleven,distribution=approximate(nresample=1000))
kruskal_test(PercSiteDry~as.factor(Cluster),data=sub.twelve,distribution=approximate(nresample=1000))

dunnTest(PercSiteDry~as.factor(Cluster),data=sub.four,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.five,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.six,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.seven,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.eight,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.nine,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.ten,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.eleven,method="bh")
dunnTest(PercSiteDry~as.factor(Cluster),data=sub.twelve,method="bh")

Sum=Summarize(PercSiteDry~Year+Cluster,data=final)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Year,y=mean,color=Cluster,data=Sum)+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.one,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.two,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.three,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.four,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.five,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.six,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.seven,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.eight,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.nine,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.ten,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.eleven,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDens~as.factor(Cluster),data=sub.twelve,distribution=approximate(nresample=1000))

dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.one,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.two,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.three,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.four,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.five,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.six,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.seven,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.eight,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.nine,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.ten,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.eleven,method="bh")
dunnTest(AvgLarvDens~as.factor(Cluster),data=sub.twelve,method="bh")

Sum=Summarize(AvgLarvDens~Year+Cluster,data=final)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Year,y=mean,color=Cluster,data=Sum)+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.one,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.two,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.three,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.four,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.five,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.six,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.seven,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.eight,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.nine,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.ten,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.eleven,distribution=approximate(nresample=1000))
kruskal_test(AvgLarvDiv~as.factor(Cluster),data=sub.twelve,distribution=approximate(nresample=1000))

dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.one,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.two,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.three,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.four,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.five,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.six,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.seven,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.eight,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.nine,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.ten,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.eleven,method="bh")
dunnTest(AvgLarvDiv~as.factor(Cluster),data=sub.twelve,method="bh")

Sum=Summarize(AvgLarvDiv~Year+Cluster,data=final)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Year,y=mean,color=Cluster,data=Sum)+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

kruskal_test(PercTreat~as.factor(Cluster),data=sub.one,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.two,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.three,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.four,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.five,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.six,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.seven,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.eight,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.nine,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.ten,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.eleven,distribution=approximate(nresample=1000))
kruskal_test(PercTreat~as.factor(Cluster),data=sub.twelve,distribution=approximate(nresample=1000))

dunnTest(PercTreat~as.factor(Cluster),data=sub.one,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.two,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.three,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.four,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.five,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.six,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.seven,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.eight,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.nine,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.ten,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.eleven,method="bh")
dunnTest(PercTreat~as.factor(Cluster),data=sub.twelve,method="bh")

Sum=Summarize(PercTreat~Year+Cluster,data=final)
Sum$se=Sum$sd/sqrt(Sum$n)
qplot(x=Year,y=mean,color=Cluster,data=Sum)+geom_errorbar(aes(ymin=mean-se,ymax=mean+se,width = 0.15))

#library(ggplot2)
#library(ggfortify)

#data.pca=prcomp(data.dist,center=TRUE,scale=TRUE)
#summary(data.pca)
#autoplot(data.pca,data=final,colour='Cluster')